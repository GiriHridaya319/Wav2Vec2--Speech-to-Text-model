{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5444e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoProcessor, Wav2Vec2FeatureExtractor,Wav2Vec2CTCTokenizer\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40c0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_new = \"finedtunemodels\"\n",
    "os.makedirs(save_dir_new, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbe409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "# print(f\"Loading model {model_id}...\")\n",
    "\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)  \n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_id, use_safetensors=True)  \n",
    "# tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"gagan3012/wav2vec2-xlsr-nepali\")  \n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# model_save_path_2 = os.path.join(save_dir_new, \"multilingual_model\")\n",
    "# os.makedirs(model_save_path_2, exist_ok=True)\n",
    "\n",
    "# model_id = \"kiranpantha/wav2vec2-large-xls-r-300m-nepali\"\n",
    "# print(f\"Loading model {model_id}...\")\n",
    "\n",
    "\n",
    "# processor = Wav2Vec2Processor.from_pretrained(model_id)  # no use_safetensors here\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_id, use_safetensors=True)  # yes here\n",
    "\n",
    "# model_save_path = os.path.join(save_dir_new, \"fine_tuned_model_kiran\")\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# processor.save_pretrained(model_save_path)\n",
    "# model.save_pretrained(model_save_path, use_safetensors=True)\n",
    "# print(f\"Saved fine_tuned_model to {model_save_path}\")\n",
    "\n",
    "# print(\"All models loaded and saved successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_id_1 = \"anish-shilpakar/wav2vec2-nepali\"\n",
    "# print(f\"Loading model {model_id_1}...\")\n",
    "\n",
    "# processor_1 = Wav2Vec2Processor.from_pretrained(model_id_1)  # no use_safetensors here\n",
    "# model_1 = Wav2Vec2ForCTC.from_pretrained(model_id_1, use_safetensors=True)  # yes here\n",
    "\n",
    "# model_save_path_1 = os.path.join(save_dir_new, \"fine_tuned_model\")\n",
    "# os.makedirs(model_save_path_1, exist_ok=True)\n",
    "\n",
    "# processor_1.save_pretrained(model_save_path_1)\n",
    "# model_1.save_pretrained(model_save_path_1, use_safetensors=True)\n",
    "# print(f\"Saved fine_tuned_model to {model_save_path_1}\")\n",
    "\n",
    "# print(\"All models loaded and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f6c21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "models_dir = \"finedtunemodels\"\n",
    "test_data_dir = \"nepali_audios\"\n",
    "transcribs_dir = \"transcribs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bebbd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [name for name in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1a9d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "def load_audio(path, target_sr=16000):\n",
    "    waveform, sr = sf.read(path)\n",
    "    if sr != target_sr:\n",
    "        waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "        waveform = waveform.squeeze(0).numpy()\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cacf9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(waveform, processor, model):\n",
    "        if waveform.ndim == 2:\n",
    "            waveform = waveform[0]  \n",
    "        \n",
    "        input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "        return transcription.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7293c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soundfile']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro\\AppData\\Local\\Temp\\ipykernel_9532\\3401228546.py:2: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "print(torchaudio.list_audio_backends())  # should now show: ['soundfile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3332a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using model: fine_tuned_model ---\n",
      "fine_tuned_model | eight.wav | Transcribed Time: 8.44s | Audio Duration: 10.30s | WER: 0.467\n",
      "Transcription: सहकार्य र सहयोग मात्र समाजमा शान्ति र सम्तृति आउछ आमि सबैदले माया जमझधारी बढाउनुपर्छ।\n",
      "Ground Truth : सहकार्य र सहयोगले मात्र समाजमा शान्ति र समृद्धि आउँछ हामी सबैले माया र समझदारी बढाउनुपर्छ।\n",
      "\n",
      "fine_tuned_model | five.wav | Transcribed Time: 3.45s | Audio Duration: 9.70s | WER: 0.375\n",
      "Transcription: स्वच्छता र स्वास्थ्य पुख्याल राख्नु हरेक नागरिको कर्त ब्यउ सफा वातावरणले मात्र स्वास्थ्य समाज सम्भव हुन्छ।\n",
      "Ground Truth : स्वच्छता र स्वास्थ्यको ख्याल राख्नु हरेक नागरिकको कर्तव्य हो सफा वातावरणले मात्र स्वस्थ समाज सम्भव हुन्छ।\n",
      "\n",
      "fine_tuned_model | four.wav | Transcribed Time: 2.16s | Audio Duration: 9.38s | WER: 0.500\n",
      "Transcription: हाम्नो देशको समनीति हाम र एकताको बलमा निर्भन गर्छ हामीले एक भएर मात्रै उजुवल भविषय बनाउन सक्छ।\n",
      "Ground Truth : हाम्रो देशको समृद्धि हाम्रो एकताको बलमा निर्भर गर्छ हामीले एक भएर मात्रै उज्जवल भविष्य बनाउन सक्छौं।\n",
      "\n",
      "fine_tuned_model | nine.wav | Transcribed Time: 2.47s | Audio Duration: 9.91s | WER: 0.467\n",
      "Transcription: कामभू नेपालि भाषा र साहित्ीय हाम्रो गौ रगो हामीले यसलाई बचाएका आगामी पुस्तामा अफतनतरण गर्नुपर्छ।\n",
      "Ground Truth : हाम्रो नेपाली भाषा र साहित्य हाम्रो गौरव हो हामीले यसलाई बचाएर आगामी पुस्तामा हस्तान्तरण गर्नुपर्छ।\n",
      "\n",
      "fine_tuned_model | one.wav | Transcribed Time: 2.67s | Audio Duration: 11.50s | WER: 0.158\n",
      "Transcription: हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई चोघार्न गरेर मात्र समाज राष्ट्रलाई अघि बढाउन सक्छौ।\n",
      "Ground Truth : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई जगेर्ना गरेर मात्र समाज र राष्ट्रलाई अघि बढाउन सक्छौं।\n",
      "\n",
      "fine_tuned_model | seven.wav | Transcribed Time: 1.80s | Audio Duration: 8.06s | WER: 0.533\n",
      "Transcription: वातावयंक संरक्षण गर्न हाम्रो दायित हो प्रकृतेसँग मेलविनाथ गरेर मात्र हामी पविष्यरै सुलक्षित बनाउन सक्छौं।\n",
      "Ground Truth : वातावरणको संरक्षण गर्नु हाम्रो दायित्व हो। प्रकृतिसँग मेलमिलाप गरेर मात्र हामी भविष्यलाई सुरक्षित बनाउन सक्छौं।\n",
      "\n",
      "fine_tuned_model | six.wav | Transcribed Time: 2.01s | Audio Duration: 9.91s | WER: 0.333\n",
      "Transcription: देशको विकासमा युवा वर्गको भूमिका अतियय महत्त्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहेनत गर्नु पर्छ।\n",
      "Ground Truth : देशको विकासमा युवा वर्गको भूमिका अतिनै महत्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहनत गर्नुपर्छ।\n",
      "\n",
      "fine_tuned_model | ten.wav | Transcribed Time: 1.58s | Audio Duration: 7.87s | WER: 0.333\n",
      "Transcription: स्वास्थ्य र शिक्षामा लगानीगर्नु मात्र वृर्गकालीन विकासको आधार प्रत्येक नागरिकले यसको महत्त्व बुझ्नुपर्छ।\n",
      "Ground Truth : स्वास्थ्य र शिक्षामा लगानी गर्नु मात्र दीर्घकालीन विकासको आधार हो प्रत्येक नागरिकले यसको महत्व बुझ्नुपर्छ।\n",
      "\n",
      "fine_tuned_model | three.wav | Transcribed Time: 2.42s | Audio Duration: 9.98s | WER: 0.375\n",
      "Transcription: मेहेनत र इमान्दारिले मात्रै सफलता प्राप्त गर्न सकिन्छ। कहिले हार नमान्नुहोज् र आफ्नो लक्ष्यमा अघि बढिरहनुहोस्।\n",
      "Ground Truth : मेहनत र इमान्दारीले मात्रै सफलता प्राप्त गर्न सकिन्छ कहिल्यै हार नमान्नुहोस् र आफ्नो लक्ष्यमा अगाडि बढिरहनुहोस्।\n",
      "\n",
      "fine_tuned_model | two.wav | Transcribed Time: 2.19s | Audio Duration: 9.55s | WER: 0.278\n",
      "Transcription: शिक्षा सबैको अधिकार र यसले मात्र मानिसको जीवनमा उझ्यालोरर्‍याउँछ। हामी सबैले शिक्षा हासी गर्न प्रयाम गर्नुपर्छ।\n",
      "Ground Truth : शिक्षा सबैको अधिकार हो र यसले मात्र मानिसको जीवनमा उज्यालो ल्याउँछ हामी सबैले शिक्षा हासिल गर्न प्रयास गर्नुपर्छ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n--- Using model: {model_name} ---\")\n",
    "    model_path = os.path.join(models_dir, model_name)\n",
    "    \n",
    "    # Load processor and model\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    # Loop through all wav files\n",
    "    for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "        if not wav_file.endswith(\".wav\"):\n",
    "            continue\n",
    "        \n",
    "        wav_path = os.path.join(test_data_dir, wav_file)\n",
    "        txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "        txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Transcription file missing for {wav_file}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load audio & ground truth text\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        audio_duration = waveform.shape[1] / sample_rate  # in seconds\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ground_truth = f.read().strip().lower()\n",
    "\n",
    "        # Transcribe & measure time\n",
    "        start_time = time.time()\n",
    "        pred_text = transcribe(waveform, processor, model)\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Calculate WER\n",
    "        error_rate = wer(ground_truth, pred_text)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{model_name} | {wav_file} | Transcribed Time: {duration:.2f}s | Audio Duration: {audio_duration:.2f}s | WER: {error_rate:.3f}\")\n",
    "        print(f\"Transcription: {pred_text}\")\n",
    "        print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"audio_file\": wav_file,\n",
    "            \"transcribed_time_sec\": duration,\n",
    "            \"audio_duration_sec\": audio_duration,\n",
    "            \"wer\": error_rate,\n",
    "            \"prediction\": pred_text,\n",
    "            \"ground_truth\": ground_truth,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fb1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_wer(results):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\n",
    "        \"total_wer\": 0.0,\n",
    "        \"count\": 0,\n",
    "        \"total_audio_duration\": 0.0,\n",
    "        \"total_transcription_time\": 0.0\n",
    "    })\n",
    "\n",
    "    for result in results:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcribed_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d367ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onx = \"finedtunemodels/fine_tuned_model/\"\n",
    "onx_model = \"fine_tuned_models.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c78ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(model_onx, onx_model):\n",
    "    print(f\"Converting mode {model_onx}\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_onx)\n",
    "    model.eval()\n",
    "\n",
    "    audio_len = 16000   \n",
    "    dummy_input = torch.randn(1, audio_len)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,     \n",
    "        dummy_input,   \n",
    "        onx_model,             \n",
    "        export_params=True,     \n",
    "        opset_version=14,         \n",
    "        do_constant_folding=True,    \n",
    "        input_names=[\"input_values\"],\n",
    "        output_names=[\"logits\"],     \n",
    "        dynamic_axes={\n",
    "            \"input_values\": {1: \"sequence_length\"},\n",
    "            \"logits\": {1: \"sequence_length\"}\n",
    "        },                         \n",
    "    )\n",
    "    print(f\"ONNX model saved at: {onx_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ad9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting mode finedtunemodels/fine_tuned_model/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:74: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved at: fine_tuned_models.onnx\n"
     ]
    }
   ],
   "source": [
    "convert_model_to_onnx(model_onx, onx_model)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_onx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12903041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = rt.InferenceSession(onx_model, sess_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d4b69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file(filepath):\n",
    "    # Load audio\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    # If not mono, convert to mono\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "        sr = 16000\n",
    "\n",
    "    # Prepare input for ONNX\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.numpy()\n",
    "\n",
    "    # Run ONNX inference\n",
    "    ort_inputs = {session.get_inputs()[0].name: input_values}\n",
    "    ort_outputs = session.run(None, ort_inputs)[0]\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = np.argmax(ort_outputs, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "619aeb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 2.33s | WER: 0.467\n",
      "Prediction   : सहकार्य र सहयोग मात्र समाजमा शान्ति र सम्तृति आउछ आमि सबैदले माया जमझधारी बढाउनुपर्छ।\n",
      "Ground Truth : सहकार्य र सहयोगले मात्र समाजमा शान्ति र समृद्धि आउँछ हामी सबैले माया र समझदारी बढाउनुपर्छ।\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 1.65s | WER: 0.375\n",
      "Prediction   : स्वच्छता र स्वास्थ्य पुख्याल राख्नु हरेक नागरिको कर्त ब्यउ सफा वातावरणले मात्र स्वास्थ्य समाज सम्भव हुन्छ।\n",
      "Ground Truth : स्वच्छता र स्वास्थ्यको ख्याल राख्नु हरेक नागरिकको कर्तव्य हो सफा वातावरणले मात्र स्वस्थ समाज सम्भव हुन्छ।\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.55s | WER: 0.500\n",
      "Prediction   : हाम्नो देशको समनीति हाम र एकताको बलमा निर्भन गर्छ हामीले एक भएर मात्रै उजुवल भविषय बनाउन सक्छ।\n",
      "Ground Truth : हाम्रो देशको समृद्धि हाम्रो एकताको बलमा निर्भर गर्छ हामीले एक भएर मात्रै उज्जवल भविष्य बनाउन सक्छौं।\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.47s | WER: 0.467\n",
      "Prediction   : कामभू नेपालि भाषा र साहित्ीय हाम्रो गौ रगो हामीले यसलाई बचाएका आगामी पुस्तामा अफतनतरण गर्नुपर्छ।\n",
      "Ground Truth : हाम्रो नेपाली भाषा र साहित्य हाम्रो गौरव हो हामीले यसलाई बचाएर आगामी पुस्तामा हस्तान्तरण गर्नुपर्छ।\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.90s | WER: 0.158\n",
      "Prediction   : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई चोघार्न गरेर मात्र समाज राष्ट्रलाई अघि बढाउन सक्छौ।\n",
      "Ground Truth : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई जगेर्ना गरेर मात्र समाज र राष्ट्रलाई अघि बढाउन सक्छौं।\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 1.20s | WER: 0.533\n",
      "Prediction   : वातावयंक संरक्षण गर्न हाम्रो दायित हो प्रकृतेसँग मेलविनाथ गरेर मात्र हामी पविष्यरै सुलक्षित बनाउन सक्छौं।\n",
      "Ground Truth : वातावरणको संरक्षण गर्नु हाम्रो दायित्व हो। प्रकृतिसँग मेलमिलाप गरेर मात्र हामी भविष्यलाई सुरक्षित बनाउन सक्छौं।\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.53s | WER: 0.333\n",
      "Prediction   : देशको विकासमा युवा वर्गको भूमिका अतियय महत्त्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहेनत गर्नु पर्छ।\n",
      "Ground Truth : देशको विकासमा युवा वर्गको भूमिका अतिनै महत्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहनत गर्नुपर्छ।\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 1.12s | WER: 0.333\n",
      "Prediction   : स्वास्थ्य र शिक्षामा लगानीगर्नु मात्र वृर्गकालीन विकासको आधार प्रत्येक नागरिकले यसको महत्त्व बुझ्नुपर्छ।\n",
      "Ground Truth : स्वास्थ्य र शिक्षामा लगानी गर्नु मात्र दीर्घकालीन विकासको आधार हो प्रत्येक नागरिकले यसको महत्व बुझ्नुपर्छ।\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.52s | WER: 0.375\n",
      "Prediction   : मेहेनत र इमान्दारिले मात्रै सफलता प्राप्त गर्न सकिन्छ। कहिले हार नमान्नुहोज् र आफ्नो लक्ष्यमा अघि बढिरहनुहोस्।\n",
      "Ground Truth : मेहनत र इमान्दारीले मात्रै सफलता प्राप्त गर्न सकिन्छ कहिल्यै हार नमान्नुहोस् र आफ्नो लक्ष्यमा अगाडि बढिरहनुहोस्।\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.44s | WER: 0.278\n",
      "Prediction   : शिक्षा सबैको अधिकार र यसले मात्र मानिसको जीवनमा उझ्यालोरर्‍याउँछ। हामी सबैले शिक्षा हासी गर्न प्रयाम गर्नुपर्छ।\n",
      "Ground Truth : शिक्षा सबैको अधिकार हो र यसले मात्र मानिसको जीवनमा उज्यालो ल्याउँछ हामी सबैले शिक्षा हासिल गर्न प्रयास गर्नुपर्छ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_onnx = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_from_file(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_onnx.append({\n",
    "        \"model\": \"onnx_FineTuned\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fba017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_o(results_onnx):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_onnx:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== ONNX Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d7a7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7bcd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "example = torch.randn(1, 160000)\n",
    "open_vino_model = ov.convert_model(model, example_input = (example,))\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a36f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model  = core.compile_model(open_vino_model, 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06a0f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file_ov(filepath):\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"np\")\n",
    "    input_values = inputs[\"input_values\"]\n",
    "\n",
    "    ov_inputs = {0: input_values}\n",
    "    ov_outputs = compiled_model(ov_inputs)\n",
    "    logits = list(ov_outputs.values())[0]\n",
    "\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d77675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 2.29s | WER: 0.467\n",
      "🔊 Prediction   : सहकार्य र सहयोग मात्र समाजमा शान्ति र सम्तृति आउछ आमि सबैदले माया जमझधारी बढाउनुपर्छ।\n",
      "📜 Ground Truth : सहकार्य र सहयोगले मात्र समाजमा शान्ति र समृद्धि आउँछ हामी सबैले माया र समझदारी बढाउनुपर्छ।\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 2.13s | WER: 0.375\n",
      "🔊 Prediction   : स्वच्छता र स्वास्थ्य पुख्याल राख्नु हरेक नागरिको कर्त ब्यउ सफा वातावरणले मात्र स्वास्थ्य समाज सम्भव हुन्छ।\n",
      "📜 Ground Truth : स्वच्छता र स्वास्थ्यको ख्याल राख्नु हरेक नागरिकको कर्तव्य हो सफा वातावरणले मात्र स्वस्थ समाज सम्भव हुन्छ।\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.40s | WER: 0.500\n",
      "🔊 Prediction   : हाम्नो देशको समनीति हाम र एकताको बलमा निर्भन गर्छ हामीले एक भएर मात्रै उजुवल भविषय बनाउन सक्छ।\n",
      "📜 Ground Truth : हाम्रो देशको समृद्धि हाम्रो एकताको बलमा निर्भर गर्छ हामीले एक भएर मात्रै उज्जवल भविष्य बनाउन सक्छौं।\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.46s | WER: 0.467\n",
      "🔊 Prediction   : कामभू नेपालि भाषा र साहित्ीय हाम्रो गौ रगो हामीले यसलाई बचाएका आगामी पुस्तामा अफतनतरण गर्नुपर्छ।\n",
      "📜 Ground Truth : हाम्रो नेपाली भाषा र साहित्य हाम्रो गौरव हो हामीले यसलाई बचाएर आगामी पुस्तामा हस्तान्तरण गर्नुपर्छ।\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.82s | WER: 0.158\n",
      "🔊 Prediction   : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई चोघार्न गरेर मात्र समाज राष्ट्रलाई अघि बढाउन सक्छौ।\n",
      "📜 Ground Truth : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई जगेर्ना गरेर मात्र समाज र राष्ट्रलाई अघि बढाउन सक्छौं।\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 1.09s | WER: 0.533\n",
      "🔊 Prediction   : वातावयंक संरक्षण गर्न हाम्रो दायित हो प्रकृतेसँग मेलविनाथ गरेर मात्र हामी पविष्यरै सुलक्षित बनाउन सक्छौं।\n",
      "📜 Ground Truth : वातावरणको संरक्षण गर्नु हाम्रो दायित्व हो। प्रकृतिसँग मेलमिलाप गरेर मात्र हामी भविष्यलाई सुरक्षित बनाउन सक्छौं।\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.15s | WER: 0.333\n",
      "🔊 Prediction   : देशको विकासमा युवा वर्गको भूमिका अतियय महत्त्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहेनत गर्नु पर्छ।\n",
      "📜 Ground Truth : देशको विकासमा युवा वर्गको भूमिका अतिनै महत्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहनत गर्नुपर्छ।\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 0.91s | WER: 0.333\n",
      "🔊 Prediction   : स्वास्थ्य र शिक्षामा लगानीगर्नु मात्र वृर्गकालीन विकासको आधार प्रत्येक नागरिकले यसको महत्त्व बुझ्नुपर्छ।\n",
      "📜 Ground Truth : स्वास्थ्य र शिक्षामा लगानी गर्नु मात्र दीर्घकालीन विकासको आधार हो प्रत्येक नागरिकले यसको महत्व बुझ्नुपर्छ।\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.19s | WER: 0.375\n",
      "🔊 Prediction   : मेहेनत र इमान्दारिले मात्रै सफलता प्राप्त गर्न सकिन्छ। कहिले हार नमान्नुहोज् र आफ्नो लक्ष्यमा अघि बढिरहनुहोस्।\n",
      "📜 Ground Truth : मेहनत र इमान्दारीले मात्रै सफलता प्राप्त गर्न सकिन्छ कहिल्यै हार नमान्नुहोस् र आफ्नो लक्ष्यमा अगाडि बढिरहनुहोस्।\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.15s | WER: 0.278\n",
      "🔊 Prediction   : शिक्षा सबैको अधिकार र यसले मात्र मानिसको जीवनमा उझ्यालोरर्‍याउँछ। हामी सबैले शिक्षा हासी गर्न प्रयाम गर्नुपर्छ।\n",
      "📜 Ground Truth : शिक्षा सबैको अधिकार हो र यसले मात्र मानिसको जीवनमा उज्यालो ल्याउँछ हामी सबैले शिक्षा हासिल गर्न प्रयास गर्नुपर्छ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results_openvino = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"❌ Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        prediction = transcribe_from_file_ov(wav_path)\n",
    "        transcription_time = time.time() - start_time\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"🔊 Prediction   : {prediction}\")\n",
    "    print(f\"📜 Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    results_openvino.append({\n",
    "        \"model\": \"openvino_FineTuned\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3bd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_open(results_openvino):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_openvino:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Openvino Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa8df960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Openvino Model Summary ===\n",
      "Model: openvino_FineTuned\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.46 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_open(results_openvino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7fc490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONNX Model Summary ===\n",
      "Model: onnx_FineTuned\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.92 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51da0d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Summary ===\n",
      "Model: fine_tuned_model\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 2.92 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb6f2f",
   "metadata": {},
   "source": [
    "### QUANTIZING pytorch fine tuned nepali model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f7eaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af0d0b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.12, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.12, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.12, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=63, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d70514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyanamic_quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype = torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80b4c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model state_dict saved to: finedtunemodels\\dynamic_quantized_finedTuned_model_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "quantized_path = os.path.join(models_dir, \"dynamic_quantized_finedTuned_model_state_dict.pth\")\n",
    "torch.save(dyanamic_quantized_model.state_dict(), quantized_path)\n",
    "print(f\"Quantized model state_dict saved to: {quantized_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9323115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1262519609 bytes\n",
      "Model size: 1204.03 MB\n",
      "Model size: 1.18 GB\n",
      "Model size is LESS than or equal to 2GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"fine_tuned_models.onnx\"\n",
    "size_bytes = os.path.getsize(model_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_mb / 1024\n",
    "\n",
    "print(f\"Model size: {size_bytes} bytes\")\n",
    "print(f\"Model size: {size_mb:.2f} MB\")\n",
    "print(f\"Model size: {size_gb:.2f} GB\")\n",
    "\n",
    "if size_gb > 2:\n",
    "    print(\"Model size is GREATER than 2GB\")\n",
    "else:\n",
    "    print(\"Model size is LESS than or equal to 2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99939cb5",
   "metadata": {},
   "source": [
    "### Dynamic quantizing ONNX fine tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30abb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model_path = \"fine_tuned_models.onnx\"\n",
    "model = onnx.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e8b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "089acaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_pre_process(\n",
    "    input_model=\"fine_tuned_models.onnx\",\n",
    "    output_model_path=\"fine_tuned_models_optimized.onnx\",\n",
    "    save_as_external_data=True,\n",
    "    all_tensors_to_one_file=True,               \n",
    "    external_data_location=\"fine_tuned_models.data\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccc6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_dynamic(\n",
    "    model_input=\"fine_tuned_models_optimized.onnx\",  \n",
    "    model_output=\"fine_tuned_models_quantized.onnx\", \n",
    "    weight_type=QuantType.QInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63989c",
   "metadata": {},
   "source": [
    "### Transcribe using quantized finetuned pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ff325aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\torch\\_utils.py:425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(quantized_path)\n",
    "dyanamic_quantized_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c38ac8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.12, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.12, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.12, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=63, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyanamic_quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e74dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_quantized_model(wav_path):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    # Convert stereo to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Remove batch/channel dimension\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = dyanamic_quantized_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e2e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 1.66s | WER: 0.467\n",
      "Prediction   : सहकार्य र सहयोग मात्र समाजमा शान्ति र सम्धृति आउछ आमि सबैदले माया जमजधारी बढाउनुपर्छ।\n",
      "Ground Truth : सहकार्य र सहयोगले मात्र समाजमा शान्ति र समृद्धि आउँछ हामी सबैले माया र समझदारी बढाउनुपर्छ।\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 1.14s | WER: 0.375\n",
      "Prediction   : स्वच्छता र स्वास्थ्य पुख्याल राख्नु हरेक नागरिको कर्त ब्यउु सफा वातावरणले मात्र स्वास्थ्य समाज सम्भव हुन्छ।\n",
      "Ground Truth : स्वच्छता र स्वास्थ्यको ख्याल राख्नु हरेक नागरिकको कर्तव्य हो सफा वातावरणले मात्र स्वस्थ समाज सम्भव हुन्छ।\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.08s | WER: 0.438\n",
      "Prediction   : हाम्नो देशको संनीति हाम रो एकताको बलमा निर्भन गर्छ हामीले एक भएर मात्रै उजुवल भविष्य बनाउन सक्छ।\n",
      "Ground Truth : हाम्रो देशको समृद्धि हाम्रो एकताको बलमा निर्भर गर्छ हामीले एक भएर मात्रै उज्जवल भविष्य बनाउन सक्छौं।\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.10s | WER: 0.400\n",
      "Prediction   : कामभू नेपालि भाषा र साहित्य हाम्रो गौ रगो हामीले यसलाई बचाएका आगामी पुस्तामा अतकतरण गर्नुपर्छ।\n",
      "Ground Truth : हाम्रो नेपाली भाषा र साहित्य हाम्रो गौरव हो हामीले यसलाई बचाएर आगामी पुस्तामा हस्तान्तरण गर्नुपर्छ।\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.34s | WER: 0.105\n",
      "Prediction   : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई चोघार्न गरेर मात्र समाज राष्ट्रलाई अघि बढाउन सक्छौं।\n",
      "Ground Truth : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई जगेर्ना गरेर मात्र समाज र राष्ट्रलाई अघि बढाउन सक्छौं।\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 0.99s | WER: 0.600\n",
      "Prediction   : वातावयंक संगक्षण गर्न हाम्रो दायित हो प्रकृतेसँग मेलविनाथ गरेर मात्र हामी भविस्यरै सुलक्षित बनाउन सक्छौं।\n",
      "Ground Truth : वातावरणको संरक्षण गर्नु हाम्रो दायित्व हो। प्रकृतिसँग मेलमिलाप गरेर मात्र हामी भविष्यलाई सुरक्षित बनाउन सक्छौं।\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.17s | WER: 0.333\n",
      "Prediction   : देशको विकासमा युवा वर्गको भूमिका अतियय महत्त्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहेनत गर्नु पर्छ।\n",
      "Ground Truth : देशको विकासमा युवा वर्गको भूमिका अतिनै महत्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहनत गर्नुपर्छ।\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 0.92s | WER: 0.333\n",
      "Prediction   : स्वास्थ्य र शिक्षामा लगानीगर्नु मात्र वृर्गकालीन विकासको आधार प्रत्येक नागरिकले यसको महत्त्व बुझ्नुपर्छ।\n",
      "Ground Truth : स्वास्थ्य र शिक्षामा लगानी गर्नु मात्र दीर्घकालीन विकासको आधार हो प्रत्येक नागरिकले यसको महत्व बुझ्नुपर्छ।\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.28s | WER: 0.375\n",
      "Prediction   : मेहेनत र इमान्दारिले मात्रै सफलता प्राप्त गर्न सकिन्छ। कहिले हार नमान्नुहोज् र आफ्नो लक्ष्यमा अघि बढिरहनुहोस्।\n",
      "Ground Truth : मेहनत र इमान्दारीले मात्रै सफलता प्राप्त गर्न सकिन्छ कहिल्यै हार नमान्नुहोस् र आफ्नो लक्ष्यमा अगाडि बढिरहनुहोस्।\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.21s | WER: 0.278\n",
      "Prediction   : शिक्षा सबैको अधिकार र यसले मात्र मानिसको जीवनमा उजझ्यालोरर्‍याउँछ। हामी सबैले शिक्षा हासी गर्न प्रयाम गर्नुपर्छ।\n",
      "Ground Truth : शिक्षा सबैको अधिकार हो र यसले मात्र मानिसको जीवनमा उज्यालो ल्याउँछ हामी सबैले शिक्षा हासिल गर्न प्रयास गर्नुपर्छ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_quantized = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_with_quantized_model(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_quantized.append({\n",
    "        \"model\": dyanamic_quantized_model,\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f18dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_wer_quantized(results_quantized):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_stats = defaultdict(lambda: {\n",
    "        \"total_wer\": 0.0,\n",
    "        \"total_audio_duration\": 0.0,\n",
    "        \"total_transcription_time\": 0.0,\n",
    "        \"count\": 0\n",
    "    })\n",
    "\n",
    "    for result in results_quantized:\n",
    "        model = result[\"model\"]\n",
    "        model_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "        model_stats[model][\"count\"] += 1\n",
    "\n",
    "    print(\"\\n=== Average Metrics per Model ===\")\n",
    "    for model, stats in model_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_duration = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_transcription_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files        : {count}\")\n",
    "        print(f\"  Average WER            : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Duration : {avg_audio_duration:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time : {avg_transcription_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb05f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics per Model ===\n",
      "Model: Wav2Vec2ForCTC(\n",
      "  (wav2vec2): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.12, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.12, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.12, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=63, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "  Number of files        : 10\n",
      "  Average WER            : 0.3704\n",
      "  Average Audio Duration : 9.62 sec\n",
      "  Average Transcription Time : 1.19 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer_quantized(results_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6ed3cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Summary ===\n",
      "Model: fine_tuned_model\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 2.92 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b19d49",
   "metadata": {},
   "source": [
    "### Transcription using finetuned ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b62a5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file(filepath):\n",
    "    # Load audio\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    # If not mono, convert to mono\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "        sr = 16000\n",
    "\n",
    "    # Prepare input for ONNX\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.numpy()\n",
    "\n",
    "    # Run ONNX inference\n",
    "    ort_inputs = {session.get_inputs()[0].name: input_values}\n",
    "    ort_outputs = session.run(None, ort_inputs)[0]\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = np.argmax(ort_outputs, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0dde013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 1.65s | WER: 0.467\n",
      "Prediction   : सहकार्य र सहयोग मात्र समाजमा शान्ति र सम्तृति आउछ आमि सबैदले माया जमझधारी बढाउनुपर्छ।\n",
      "Ground Truth : सहकार्य र सहयोगले मात्र समाजमा शान्ति र समृद्धि आउँछ हामी सबैले माया र समझदारी बढाउनुपर्छ।\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 1.52s | WER: 0.375\n",
      "Prediction   : स्वच्छता र स्वास्थ्य पुख्याल राख्नु हरेक नागरिको कर्त ब्यउ सफा वातावरणले मात्र स्वास्थ्य समाज सम्भव हुन्छ।\n",
      "Ground Truth : स्वच्छता र स्वास्थ्यको ख्याल राख्नु हरेक नागरिकको कर्तव्य हो सफा वातावरणले मात्र स्वस्थ समाज सम्भव हुन्छ।\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.45s | WER: 0.500\n",
      "Prediction   : हाम्नो देशको समनीति हाम र एकताको बलमा निर्भन गर्छ हामीले एक भएर मात्रै उजुवल भविषय बनाउन सक्छ।\n",
      "Ground Truth : हाम्रो देशको समृद्धि हाम्रो एकताको बलमा निर्भर गर्छ हामीले एक भएर मात्रै उज्जवल भविष्य बनाउन सक्छौं।\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.53s | WER: 0.467\n",
      "Prediction   : कामभू नेपालि भाषा र साहित्ीय हाम्रो गौ रगो हामीले यसलाई बचाएका आगामी पुस्तामा अफतनतरण गर्नुपर्छ।\n",
      "Ground Truth : हाम्रो नेपाली भाषा र साहित्य हाम्रो गौरव हो हामीले यसलाई बचाएर आगामी पुस्तामा हस्तान्तरण गर्नुपर्छ।\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.84s | WER: 0.158\n",
      "Prediction   : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई चोघार्न गरेर मात्र समाज राष्ट्रलाई अघि बढाउन सक्छौ।\n",
      "Ground Truth : हाम्रो भाषा संस्कृति र संस्कार हाम्रो पहिचान हुन् हामीले यिनलाई जगेर्ना गरेर मात्र समाज र राष्ट्रलाई अघि बढाउन सक्छौं।\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 1.23s | WER: 0.533\n",
      "Prediction   : वातावयंक संरक्षण गर्न हाम्रो दायित हो प्रकृतेसँग मेलविनाथ गरेर मात्र हामी पविष्यरै सुलक्षित बनाउन सक्छौं।\n",
      "Ground Truth : वातावरणको संरक्षण गर्नु हाम्रो दायित्व हो। प्रकृतिसँग मेलमिलाप गरेर मात्र हामी भविष्यलाई सुरक्षित बनाउन सक्छौं।\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.47s | WER: 0.333\n",
      "Prediction   : देशको विकासमा युवा वर्गको भूमिका अतियय महत्त्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहेनत गर्नु पर्छ।\n",
      "Ground Truth : देशको विकासमा युवा वर्गको भूमिका अतिनै महत्वपूर्ण छ हामीले देशप्रति जिम्मेवारी बोध गरेर मेहनत गर्नुपर्छ।\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 1.14s | WER: 0.333\n",
      "Prediction   : स्वास्थ्य र शिक्षामा लगानीगर्नु मात्र वृर्गकालीन विकासको आधार प्रत्येक नागरिकले यसको महत्त्व बुझ्नुपर्छ।\n",
      "Ground Truth : स्वास्थ्य र शिक्षामा लगानी गर्नु मात्र दीर्घकालीन विकासको आधार हो प्रत्येक नागरिकले यसको महत्व बुझ्नुपर्छ।\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.48s | WER: 0.375\n",
      "Prediction   : मेहेनत र इमान्दारिले मात्रै सफलता प्राप्त गर्न सकिन्छ। कहिले हार नमान्नुहोज् र आफ्नो लक्ष्यमा अघि बढिरहनुहोस्।\n",
      "Ground Truth : मेहनत र इमान्दारीले मात्रै सफलता प्राप्त गर्न सकिन्छ कहिल्यै हार नमान्नुहोस् र आफ्नो लक्ष्यमा अगाडि बढिरहनुहोस्।\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.48s | WER: 0.278\n",
      "Prediction   : शिक्षा सबैको अधिकार र यसले मात्र मानिसको जीवनमा उझ्यालोरर्‍याउँछ। हामी सबैले शिक्षा हासी गर्न प्रयाम गर्नुपर्छ।\n",
      "Ground Truth : शिक्षा सबैको अधिकार हो र यसले मात्र मानिसको जीवनमा उज्यालो ल्याउँछ हामी सबैले शिक्षा हासिल गर्न प्रयास गर्नुपर्छ।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_onnx_quantized = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_from_file(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_onnx_quantized.append({\n",
    "        \"model\": \"fine_tuned_models_quantized.onnx\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91d9fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_quantized(results_onnx_quantized):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_onnx_quantized:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Quantized ONNX Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "583ec230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quantized ONNX Model Summary ===\n",
      "Model: fine_tuned_models_quantized.onnx\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.48 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_quantized(results_onnx_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "901a57aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONNX Model Summary ===\n",
      "Model: onnx_FineTuned\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.57 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34d130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
