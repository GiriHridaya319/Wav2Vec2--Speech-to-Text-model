{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5444e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoProcessor, Wav2Vec2FeatureExtractor,Wav2Vec2CTCTokenizer\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40c0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_new = \"finedtunemodels\"\n",
    "os.makedirs(save_dir_new, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbe409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "# print(f\"Loading model {model_id}...\")\n",
    "\n",
    "# feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)  \n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_id, use_safetensors=True)  \n",
    "# tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"gagan3012/wav2vec2-xlsr-nepali\")  \n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# model_save_path_2 = os.path.join(save_dir_new, \"multilingual_model\")\n",
    "# os.makedirs(model_save_path_2, exist_ok=True)\n",
    "\n",
    "# model_id = \"kiranpantha/wav2vec2-large-xls-r-300m-nepali\"\n",
    "# print(f\"Loading model {model_id}...\")\n",
    "\n",
    "\n",
    "# processor = Wav2Vec2Processor.from_pretrained(model_id)  # no use_safetensors here\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(model_id, use_safetensors=True)  # yes here\n",
    "\n",
    "# model_save_path = os.path.join(save_dir_new, \"fine_tuned_model_kiran\")\n",
    "# os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# processor.save_pretrained(model_save_path)\n",
    "# model.save_pretrained(model_save_path, use_safetensors=True)\n",
    "# print(f\"Saved fine_tuned_model to {model_save_path}\")\n",
    "\n",
    "# print(\"All models loaded and saved successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_id_1 = \"anish-shilpakar/wav2vec2-nepali\"\n",
    "# print(f\"Loading model {model_id_1}...\")\n",
    "\n",
    "# processor_1 = Wav2Vec2Processor.from_pretrained(model_id_1)  # no use_safetensors here\n",
    "# model_1 = Wav2Vec2ForCTC.from_pretrained(model_id_1, use_safetensors=True)  # yes here\n",
    "\n",
    "# model_save_path_1 = os.path.join(save_dir_new, \"fine_tuned_model\")\n",
    "# os.makedirs(model_save_path_1, exist_ok=True)\n",
    "\n",
    "# processor_1.save_pretrained(model_save_path_1)\n",
    "# model_1.save_pretrained(model_save_path_1, use_safetensors=True)\n",
    "# print(f\"Saved fine_tuned_model to {model_save_path_1}\")\n",
    "\n",
    "# print(\"All models loaded and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f6c21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "models_dir = \"finedtunemodels\"\n",
    "test_data_dir = \"nepali_audios\"\n",
    "transcribs_dir = \"transcribs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bebbd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [name for name in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1a9d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "def load_audio(path, target_sr=16000):\n",
    "    waveform, sr = sf.read(path)\n",
    "    if sr != target_sr:\n",
    "        waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "        waveform = waveform.squeeze(0).numpy()\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cacf9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(waveform, processor, model):\n",
    "        if waveform.ndim == 2:\n",
    "            waveform = waveform[0]  \n",
    "        \n",
    "        input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "        return transcription.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7293c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soundfile']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro\\AppData\\Local\\Temp\\ipykernel_9532\\3401228546.py:2: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "print(torchaudio.list_audio_backends())  # should now show: ['soundfile']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3332a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using model: fine_tuned_model ---\n",
      "fine_tuned_model | eight.wav | Transcribed Time: 8.44s | Audio Duration: 10.30s | WER: 0.467\n",
      "Transcription: à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤— à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥à¤¤à¥ƒà¤¤à¤¿ à¤†à¤‰à¤› à¤†à¤®à¤¿ à¤¸à¤¬à¥ˆà¤¦à¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤œà¤®à¤à¤§à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤—à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤†à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤° à¤¸à¤®à¤à¤¦à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "fine_tuned_model | five.wav | Transcribed Time: 3.45s | Audio Duration: 9.70s | WER: 0.375\n",
      "Transcription: à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤ªà¥à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹ à¤•à¤°à¥à¤¤ à¤¬à¥à¤¯à¤‰ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯à¤•à¥‹ à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤•à¥‹ à¤•à¤°à¥à¤¤à¤µà¥à¤¯ à¤¹à¥‹ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¸à¥à¤¥ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "\n",
      "fine_tuned_model | four.wav | Transcribed Time: 2.16s | Audio Duration: 9.38s | WER: 0.500\n",
      "Transcription: à¤¹à¤¾à¤®à¥à¤¨à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¤¨à¥€à¤¤à¤¿ à¤¹à¤¾à¤® à¤° à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤¨ à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤µà¤² à¤­à¤µà¤¿à¤·à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤° à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤œà¤µà¤² à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "fine_tuned_model | nine.wav | Transcribed Time: 2.47s | Audio Duration: 9.91s | WER: 0.467\n",
      "Transcription: à¤•à¤¾à¤®à¤­à¥‚ à¤¨à¥‡à¤ªà¤¾à¤²à¤¿ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¥€à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œ à¤°à¤—à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤•à¤¾ à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤…à¤«à¤¤à¤¨à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œà¤°à¤µ à¤¹à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤° à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤¹à¤¸à¥à¤¤à¤¾à¤¨à¥à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "fine_tuned_model | one.wav | Transcribed Time: 2.67s | Audio Duration: 11.50s | WER: 0.158\n",
      "Transcription: à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤šà¥‹à¤˜à¤¾à¤°à¥à¤¨ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤œà¤—à¥‡à¤°à¥à¤¨à¤¾ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤° à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "fine_tuned_model | seven.wav | Transcribed Time: 1.80s | Audio Duration: 8.06s | WER: 0.533\n",
      "Transcription: à¤µà¤¾à¤¤à¤¾à¤µà¤¯à¤‚à¤• à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤ à¤¹à¥‹ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¥‡à¤¸à¤à¤— à¤®à¥‡à¤²à¤µà¤¿à¤¨à¤¾à¤¥ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤ªà¤µà¤¿à¤·à¥à¤¯à¤°à¥ˆ à¤¸à¥à¤²à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "Ground Truth : à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤•à¥‹ à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨à¥ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤à¥à¤µ à¤¹à¥‹à¥¤ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¤¿à¤¸à¤à¤— à¤®à¥‡à¤²à¤®à¤¿à¤²à¤¾à¤ª à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤­à¤µà¤¿à¤·à¥à¤¯à¤²à¤¾à¤ˆ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "fine_tuned_model | six.wav | Transcribed Time: 2.01s | Audio Duration: 9.91s | WER: 0.333\n",
      "Transcription: à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¯à¤¯ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥ à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¨à¥ˆ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "fine_tuned_model | ten.wav | Transcribed Time: 1.58s | Audio Duration: 7.87s | WER: 0.333\n",
      "Transcription: à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤µà¥ƒà¤°à¥à¤—à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€ à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤¦à¥€à¤°à¥à¤˜à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤¹à¥‹ à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "fine_tuned_model | three.wav | Transcribed Time: 2.42s | Audio Duration: 9.98s | WER: 0.375\n",
      "Transcription: à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¤¿à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤›à¥¤ à¤•à¤¹à¤¿à¤²à¥‡ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤œà¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "Ground Truth : à¤®à¥‡à¤¹à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¥€à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤› à¤•à¤¹à¤¿à¤²à¥à¤¯à¥ˆ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤¸à¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤—à¤¾à¤¡à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "\n",
      "fine_tuned_model | two.wav | Transcribed Time: 2.19s | Audio Duration: 9.55s | WER: 0.278\n",
      "Transcription: à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤à¥à¤¯à¤¾à¤²à¥‹à¤°à¤°à¥â€à¤¯à¤¾à¤‰à¤à¤›à¥¤ à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¥€ à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤® à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¹à¥‹ à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤œà¥à¤¯à¤¾à¤²à¥‹ à¤²à¥à¤¯à¤¾à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¤¿à¤² à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n--- Using model: {model_name} ---\")\n",
    "    model_path = os.path.join(models_dir, model_name)\n",
    "    \n",
    "    # Load processor and model\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    # Loop through all wav files\n",
    "    for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "        if not wav_file.endswith(\".wav\"):\n",
    "            continue\n",
    "        \n",
    "        wav_path = os.path.join(test_data_dir, wav_file)\n",
    "        txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "        txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Transcription file missing for {wav_file}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load audio & ground truth text\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        audio_duration = waveform.shape[1] / sample_rate  # in seconds\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ground_truth = f.read().strip().lower()\n",
    "\n",
    "        # Transcribe & measure time\n",
    "        start_time = time.time()\n",
    "        pred_text = transcribe(waveform, processor, model)\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Calculate WER\n",
    "        error_rate = wer(ground_truth, pred_text)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{model_name} | {wav_file} | Transcribed Time: {duration:.2f}s | Audio Duration: {audio_duration:.2f}s | WER: {error_rate:.3f}\")\n",
    "        print(f\"Transcription: {pred_text}\")\n",
    "        print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"audio_file\": wav_file,\n",
    "            \"transcribed_time_sec\": duration,\n",
    "            \"audio_duration_sec\": audio_duration,\n",
    "            \"wer\": error_rate,\n",
    "            \"prediction\": pred_text,\n",
    "            \"ground_truth\": ground_truth,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fb1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_wer(results):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\n",
    "        \"total_wer\": 0.0,\n",
    "        \"count\": 0,\n",
    "        \"total_audio_duration\": 0.0,\n",
    "        \"total_transcription_time\": 0.0\n",
    "    })\n",
    "\n",
    "    for result in results:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcribed_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d367ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onx = \"finedtunemodels/fine_tuned_model/\"\n",
    "onx_model = \"fine_tuned_models.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c78ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(model_onx, onx_model):\n",
    "    print(f\"Converting mode {model_onx}\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_onx)\n",
    "    model.eval()\n",
    "\n",
    "    audio_len = 16000   \n",
    "    dummy_input = torch.randn(1, audio_len)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,     \n",
    "        dummy_input,   \n",
    "        onx_model,             \n",
    "        export_params=True,     \n",
    "        opset_version=14,         \n",
    "        do_constant_folding=True,    \n",
    "        input_names=[\"input_values\"],\n",
    "        output_names=[\"logits\"],     \n",
    "        dynamic_axes={\n",
    "            \"input_values\": {1: \"sequence_length\"},\n",
    "            \"logits\": {1: \"sequence_length\"}\n",
    "        },                         \n",
    "    )\n",
    "    print(f\"ONNX model saved at: {onx_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ad9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting mode finedtunemodels/fine_tuned_model/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:74: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved at: fine_tuned_models.onnx\n"
     ]
    }
   ],
   "source": [
    "convert_model_to_onnx(model_onx, onx_model)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_onx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12903041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = rt.InferenceSession(onx_model, sess_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d4b69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file(filepath):\n",
    "    # Load audio\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    # If not mono, convert to mono\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "        sr = 16000\n",
    "\n",
    "    # Prepare input for ONNX\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.numpy()\n",
    "\n",
    "    # Run ONNX inference\n",
    "    ort_inputs = {session.get_inputs()[0].name: input_values}\n",
    "    ort_outputs = session.run(None, ort_inputs)[0]\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = np.argmax(ort_outputs, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "619aeb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 2.33s | WER: 0.467\n",
      "Prediction   : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤— à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥à¤¤à¥ƒà¤¤à¤¿ à¤†à¤‰à¤› à¤†à¤®à¤¿ à¤¸à¤¬à¥ˆà¤¦à¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤œà¤®à¤à¤§à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤—à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤†à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤° à¤¸à¤®à¤à¤¦à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 1.65s | WER: 0.375\n",
      "Prediction   : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤ªà¥à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹ à¤•à¤°à¥à¤¤ à¤¬à¥à¤¯à¤‰ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯à¤•à¥‹ à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤•à¥‹ à¤•à¤°à¥à¤¤à¤µà¥à¤¯ à¤¹à¥‹ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¸à¥à¤¥ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.55s | WER: 0.500\n",
      "Prediction   : à¤¹à¤¾à¤®à¥à¤¨à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¤¨à¥€à¤¤à¤¿ à¤¹à¤¾à¤® à¤° à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤¨ à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤µà¤² à¤­à¤µà¤¿à¤·à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤° à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤œà¤µà¤² à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.47s | WER: 0.467\n",
      "Prediction   : à¤•à¤¾à¤®à¤­à¥‚ à¤¨à¥‡à¤ªà¤¾à¤²à¤¿ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¥€à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œ à¤°à¤—à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤•à¤¾ à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤…à¤«à¤¤à¤¨à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œà¤°à¤µ à¤¹à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤° à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤¹à¤¸à¥à¤¤à¤¾à¤¨à¥à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.90s | WER: 0.158\n",
      "Prediction   : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤šà¥‹à¤˜à¤¾à¤°à¥à¤¨ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤œà¤—à¥‡à¤°à¥à¤¨à¤¾ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤° à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 1.20s | WER: 0.533\n",
      "Prediction   : à¤µà¤¾à¤¤à¤¾à¤µà¤¯à¤‚à¤• à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤ à¤¹à¥‹ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¥‡à¤¸à¤à¤— à¤®à¥‡à¤²à¤µà¤¿à¤¨à¤¾à¤¥ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤ªà¤µà¤¿à¤·à¥à¤¯à¤°à¥ˆ à¤¸à¥à¤²à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "Ground Truth : à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤•à¥‹ à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨à¥ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤à¥à¤µ à¤¹à¥‹à¥¤ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¤¿à¤¸à¤à¤— à¤®à¥‡à¤²à¤®à¤¿à¤²à¤¾à¤ª à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤­à¤µà¤¿à¤·à¥à¤¯à¤²à¤¾à¤ˆ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.53s | WER: 0.333\n",
      "Prediction   : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¯à¤¯ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥ à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¨à¥ˆ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 1.12s | WER: 0.333\n",
      "Prediction   : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤µà¥ƒà¤°à¥à¤—à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€ à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤¦à¥€à¤°à¥à¤˜à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤¹à¥‹ à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.52s | WER: 0.375\n",
      "Prediction   : à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¤¿à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤›à¥¤ à¤•à¤¹à¤¿à¤²à¥‡ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤œà¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "Ground Truth : à¤®à¥‡à¤¹à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¥€à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤› à¤•à¤¹à¤¿à¤²à¥à¤¯à¥ˆ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤¸à¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤—à¤¾à¤¡à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.44s | WER: 0.278\n",
      "Prediction   : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤à¥à¤¯à¤¾à¤²à¥‹à¤°à¤°à¥â€à¤¯à¤¾à¤‰à¤à¤›à¥¤ à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¥€ à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤® à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¹à¥‹ à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤œà¥à¤¯à¤¾à¤²à¥‹ à¤²à¥à¤¯à¤¾à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¤¿à¤² à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_onnx = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_from_file(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_onnx.append({\n",
    "        \"model\": \"onnx_FineTuned\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fba017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_o(results_onnx):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_onnx:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== ONNX Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d7a7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7bcd4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "example = torch.randn(1, 160000)\n",
    "open_vino_model = ov.convert_model(model, example_input = (example,))\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a36f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model  = core.compile_model(open_vino_model, 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06a0f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file_ov(filepath):\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"np\")\n",
    "    input_values = inputs[\"input_values\"]\n",
    "\n",
    "    ov_inputs = {0: input_values}\n",
    "    ov_outputs = compiled_model(ov_inputs)\n",
    "    logits = list(ov_outputs.values())[0]\n",
    "\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d77675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 2.29s | WER: 0.467\n",
      "ðŸ”Š Prediction   : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤— à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥à¤¤à¥ƒà¤¤à¤¿ à¤†à¤‰à¤› à¤†à¤®à¤¿ à¤¸à¤¬à¥ˆà¤¦à¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤œà¤®à¤à¤§à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤—à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤†à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤° à¤¸à¤®à¤à¤¦à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 2.13s | WER: 0.375\n",
      "ðŸ”Š Prediction   : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤ªà¥à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹ à¤•à¤°à¥à¤¤ à¤¬à¥à¤¯à¤‰ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯à¤•à¥‹ à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤•à¥‹ à¤•à¤°à¥à¤¤à¤µà¥à¤¯ à¤¹à¥‹ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¸à¥à¤¥ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.40s | WER: 0.500\n",
      "ðŸ”Š Prediction   : à¤¹à¤¾à¤®à¥à¤¨à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¤¨à¥€à¤¤à¤¿ à¤¹à¤¾à¤® à¤° à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤¨ à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤µà¤² à¤­à¤µà¤¿à¤·à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤° à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤œà¤µà¤² à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.46s | WER: 0.467\n",
      "ðŸ”Š Prediction   : à¤•à¤¾à¤®à¤­à¥‚ à¤¨à¥‡à¤ªà¤¾à¤²à¤¿ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¥€à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œ à¤°à¤—à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤•à¤¾ à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤…à¤«à¤¤à¤¨à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œà¤°à¤µ à¤¹à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤° à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤¹à¤¸à¥à¤¤à¤¾à¤¨à¥à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.82s | WER: 0.158\n",
      "ðŸ”Š Prediction   : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤šà¥‹à¤˜à¤¾à¤°à¥à¤¨ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¥¤\n",
      "ðŸ“œ Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤œà¤—à¥‡à¤°à¥à¤¨à¤¾ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤° à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 1.09s | WER: 0.533\n",
      "ðŸ”Š Prediction   : à¤µà¤¾à¤¤à¤¾à¤µà¤¯à¤‚à¤• à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤ à¤¹à¥‹ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¥‡à¤¸à¤à¤— à¤®à¥‡à¤²à¤µà¤¿à¤¨à¤¾à¤¥ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤ªà¤µà¤¿à¤·à¥à¤¯à¤°à¥ˆ à¤¸à¥à¤²à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "ðŸ“œ Ground Truth : à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤•à¥‹ à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨à¥ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤à¥à¤µ à¤¹à¥‹à¥¤ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¤¿à¤¸à¤à¤— à¤®à¥‡à¤²à¤®à¤¿à¤²à¤¾à¤ª à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤­à¤µà¤¿à¤·à¥à¤¯à¤²à¤¾à¤ˆ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.15s | WER: 0.333\n",
      "ðŸ”Š Prediction   : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¯à¤¯ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥ à¤ªà¤°à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¨à¥ˆ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 0.91s | WER: 0.333\n",
      "ðŸ”Š Prediction   : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤µà¥ƒà¤°à¥à¤—à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€ à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤¦à¥€à¤°à¥à¤˜à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤¹à¥‹ à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.19s | WER: 0.375\n",
      "ðŸ”Š Prediction   : à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¤¿à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤›à¥¤ à¤•à¤¹à¤¿à¤²à¥‡ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤œà¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "ðŸ“œ Ground Truth : à¤®à¥‡à¤¹à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¥€à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤› à¤•à¤¹à¤¿à¤²à¥à¤¯à¥ˆ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤¸à¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤—à¤¾à¤¡à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.15s | WER: 0.278\n",
      "ðŸ”Š Prediction   : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤à¥à¤¯à¤¾à¤²à¥‹à¤°à¤°à¥â€à¤¯à¤¾à¤‰à¤à¤›à¥¤ à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¥€ à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤® à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "ðŸ“œ Ground Truth : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¹à¥‹ à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤œà¥à¤¯à¤¾à¤²à¥‹ à¤²à¥à¤¯à¤¾à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¤¿à¤² à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results_openvino = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"âŒ Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        prediction = transcribe_from_file_ov(wav_path)\n",
    "        transcription_time = time.time() - start_time\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"ðŸ”Š Prediction   : {prediction}\")\n",
    "    print(f\"ðŸ“œ Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    results_openvino.append({\n",
    "        \"model\": \"openvino_FineTuned\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba3bd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_open(results_openvino):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_openvino:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Openvino Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa8df960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Openvino Model Summary ===\n",
      "Model: openvino_FineTuned\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.46 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_open(results_openvino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7fc490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONNX Model Summary ===\n",
      "Model: onnx_FineTuned\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.92 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51da0d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Summary ===\n",
      "Model: fine_tuned_model\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 2.92 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb6f2f",
   "metadata": {},
   "source": [
    "### QUANTIZING pytorch fine tuned nepali model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f7eaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af0d0b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.12, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.12, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.12, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=63, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d70514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyanamic_quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype = torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80b4c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model state_dict saved to: finedtunemodels\\dynamic_quantized_finedTuned_model_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "quantized_path = os.path.join(models_dir, \"dynamic_quantized_finedTuned_model_state_dict.pth\")\n",
    "torch.save(dyanamic_quantized_model.state_dict(), quantized_path)\n",
    "print(f\"Quantized model state_dict saved to: {quantized_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9323115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1262519609 bytes\n",
      "Model size: 1204.03 MB\n",
      "Model size: 1.18 GB\n",
      "Model size is LESS than or equal to 2GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"fine_tuned_models.onnx\"\n",
    "size_bytes = os.path.getsize(model_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_mb / 1024\n",
    "\n",
    "print(f\"Model size: {size_bytes} bytes\")\n",
    "print(f\"Model size: {size_mb:.2f} MB\")\n",
    "print(f\"Model size: {size_gb:.2f} GB\")\n",
    "\n",
    "if size_gb > 2:\n",
    "    print(\"Model size is GREATER than 2GB\")\n",
    "else:\n",
    "    print(\"Model size is LESS than or equal to 2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99939cb5",
   "metadata": {},
   "source": [
    "### Dynamic quantizing ONNX fine tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30abb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model_path = \"fine_tuned_models.onnx\"\n",
    "model = onnx.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e8b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "089acaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_pre_process(\n",
    "    input_model=\"fine_tuned_models.onnx\",\n",
    "    output_model_path=\"fine_tuned_models_optimized.onnx\",\n",
    "    save_as_external_data=True,\n",
    "    all_tensors_to_one_file=True,               \n",
    "    external_data_location=\"fine_tuned_models.data\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ccc6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_dynamic(\n",
    "    model_input=\"fine_tuned_models_optimized.onnx\",  \n",
    "    model_output=\"fine_tuned_models_quantized.onnx\", \n",
    "    weight_type=QuantType.QInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63989c",
   "metadata": {},
   "source": [
    "### Transcribe using quantized finetuned pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ff325aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\torch\\_utils.py:425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(quantized_path)\n",
    "dyanamic_quantized_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c38ac8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.12, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.12, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.12, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=63, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyanamic_quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e74dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_quantized_model(wav_path):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    # Convert stereo to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Remove batch/channel dimension\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = dyanamic_quantized_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80e2e0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 1.66s | WER: 0.467\n",
      "Prediction   : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤— à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥à¤§à¥ƒà¤¤à¤¿ à¤†à¤‰à¤› à¤†à¤®à¤¿ à¤¸à¤¬à¥ˆà¤¦à¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤œà¤®à¤œà¤§à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤—à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤†à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤° à¤¸à¤®à¤à¤¦à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 1.14s | WER: 0.375\n",
      "Prediction   : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤ªà¥à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹ à¤•à¤°à¥à¤¤ à¤¬à¥à¤¯à¤‰à¥ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯à¤•à¥‹ à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤•à¥‹ à¤•à¤°à¥à¤¤à¤µà¥à¤¯ à¤¹à¥‹ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¸à¥à¤¥ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.08s | WER: 0.438\n",
      "Prediction   : à¤¹à¤¾à¤®à¥à¤¨à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤‚à¤¨à¥€à¤¤à¤¿ à¤¹à¤¾à¤® à¤°à¥‹ à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤¨ à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤µà¤² à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤° à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤œà¤µà¤² à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.10s | WER: 0.400\n",
      "Prediction   : à¤•à¤¾à¤®à¤­à¥‚ à¤¨à¥‡à¤ªà¤¾à¤²à¤¿ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œ à¤°à¤—à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤•à¤¾ à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤…à¤¤à¤•à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œà¤°à¤µ à¤¹à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤° à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤¹à¤¸à¥à¤¤à¤¾à¤¨à¥à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.34s | WER: 0.105\n",
      "Prediction   : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤šà¥‹à¤˜à¤¾à¤°à¥à¤¨ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤œà¤—à¥‡à¤°à¥à¤¨à¤¾ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤° à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 0.99s | WER: 0.600\n",
      "Prediction   : à¤µà¤¾à¤¤à¤¾à¤µà¤¯à¤‚à¤• à¤¸à¤‚à¤—à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤ à¤¹à¥‹ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¥‡à¤¸à¤à¤— à¤®à¥‡à¤²à¤µà¤¿à¤¨à¤¾à¤¥ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤­à¤µà¤¿à¤¸à¥à¤¯à¤°à¥ˆ à¤¸à¥à¤²à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "Ground Truth : à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤•à¥‹ à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨à¥ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤à¥à¤µ à¤¹à¥‹à¥¤ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¤¿à¤¸à¤à¤— à¤®à¥‡à¤²à¤®à¤¿à¤²à¤¾à¤ª à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤­à¤µà¤¿à¤·à¥à¤¯à¤²à¤¾à¤ˆ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.17s | WER: 0.333\n",
      "Prediction   : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¯à¤¯ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥ à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¨à¥ˆ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 0.92s | WER: 0.333\n",
      "Prediction   : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤µà¥ƒà¤°à¥à¤—à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€ à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤¦à¥€à¤°à¥à¤˜à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤¹à¥‹ à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.28s | WER: 0.375\n",
      "Prediction   : à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¤¿à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤›à¥¤ à¤•à¤¹à¤¿à¤²à¥‡ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤œà¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "Ground Truth : à¤®à¥‡à¤¹à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¥€à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤› à¤•à¤¹à¤¿à¤²à¥à¤¯à¥ˆ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤¸à¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤—à¤¾à¤¡à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.21s | WER: 0.278\n",
      "Prediction   : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤œà¤à¥à¤¯à¤¾à¤²à¥‹à¤°à¤°à¥â€à¤¯à¤¾à¤‰à¤à¤›à¥¤ à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¥€ à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤® à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¹à¥‹ à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤œà¥à¤¯à¤¾à¤²à¥‹ à¤²à¥à¤¯à¤¾à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¤¿à¤² à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_quantized = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_with_quantized_model(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_quantized.append({\n",
    "        \"model\": dyanamic_quantized_model,\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f18dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_wer_quantized(results_quantized):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_stats = defaultdict(lambda: {\n",
    "        \"total_wer\": 0.0,\n",
    "        \"total_audio_duration\": 0.0,\n",
    "        \"total_transcription_time\": 0.0,\n",
    "        \"count\": 0\n",
    "    })\n",
    "\n",
    "    for result in results_quantized:\n",
    "        model = result[\"model\"]\n",
    "        model_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "        model_stats[model][\"count\"] += 1\n",
    "\n",
    "    print(\"\\n=== Average Metrics per Model ===\")\n",
    "    for model, stats in model_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_duration = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_transcription_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files        : {count}\")\n",
    "        print(f\"  Average WER            : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Duration : {avg_audio_duration:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time : {avg_transcription_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb05f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics per Model ===\n",
      "Model: Wav2Vec2ForCTC(\n",
      "  (wav2vec2): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.12, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.12, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.12, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=63, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "  Number of files        : 10\n",
      "  Average WER            : 0.3704\n",
      "  Average Audio Duration : 9.62 sec\n",
      "  Average Transcription Time : 1.19 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer_quantized(results_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6ed3cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Summary ===\n",
      "Model: fine_tuned_model\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 2.92 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b19d49",
   "metadata": {},
   "source": [
    "### Transcription using finetuned ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b62a5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file(filepath):\n",
    "    # Load audio\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    # If not mono, convert to mono\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "        sr = 16000\n",
    "\n",
    "    # Prepare input for ONNX\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.numpy()\n",
    "\n",
    "    # Run ONNX inference\n",
    "    ort_inputs = {session.get_inputs()[0].name: input_values}\n",
    "    ort_outputs = session.run(None, ort_inputs)[0]\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = np.argmax(ort_outputs, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0dde013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight.wav | Audio Duration: 10.30s | Transcription Time: 1.65s | WER: 0.467\n",
      "Prediction   : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤— à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥à¤¤à¥ƒà¤¤à¤¿ à¤†à¤‰à¤› à¤†à¤®à¤¿ à¤¸à¤¬à¥ˆà¤¦à¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤œà¤®à¤à¤§à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¤¹à¤•à¤¾à¤°à¥à¤¯ à¤° à¤¸à¤¹à¤¯à¥‹à¤—à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œà¤®à¤¾ à¤¶à¤¾à¤¨à¥à¤¤à¤¿ à¤° à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤†à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤®à¤¾à¤¯à¤¾ à¤° à¤¸à¤®à¤à¤¦à¤¾à¤°à¥€ à¤¬à¤¢à¤¾à¤‰à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "five.wav | Audio Duration: 9.70s | Transcription Time: 1.52s | WER: 0.375\n",
      "Prediction   : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤ªà¥à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¥‹ à¤•à¤°à¥à¤¤ à¤¬à¥à¤¯à¤‰ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤šà¥à¤›à¤¤à¤¾ à¤° à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯à¤•à¥‹ à¤–à¥à¤¯à¤¾à¤² à¤°à¤¾à¤–à¥à¤¨à¥ à¤¹à¤°à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤•à¥‹ à¤•à¤°à¥à¤¤à¤µà¥à¤¯ à¤¹à¥‹ à¤¸à¤«à¤¾ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤¸à¥à¤µà¤¸à¥à¤¥ à¤¸à¤®à¤¾à¤œ à¤¸à¤®à¥à¤­à¤µ à¤¹à¥à¤¨à¥à¤›à¥¤\n",
      "\n",
      "four.wav | Audio Duration: 9.38s | Transcription Time: 1.45s | WER: 0.500\n",
      "Prediction   : à¤¹à¤¾à¤®à¥à¤¨à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¤¨à¥€à¤¤à¤¿ à¤¹à¤¾à¤® à¤° à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤¨ à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤µà¤² à¤­à¤µà¤¿à¤·à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¥‡à¤¶à¤•à¥‹ à¤¸à¤®à¥ƒà¤¦à¥à¤§à¤¿ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤à¤•à¤¤à¤¾à¤•à¥‹ à¤¬à¤²à¤®à¤¾ à¤¨à¤¿à¤°à¥à¤­à¤° à¤—à¤°à¥à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤à¤• à¤­à¤à¤° à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤‰à¤œà¥à¤œà¤µà¤² à¤­à¤µà¤¿à¤·à¥à¤¯ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "nine.wav | Audio Duration: 9.91s | Transcription Time: 1.53s | WER: 0.467\n",
      "Prediction   : à¤•à¤¾à¤®à¤­à¥‚ à¤¨à¥‡à¤ªà¤¾à¤²à¤¿ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¥€à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œ à¤°à¤—à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤•à¤¾ à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤…à¤«à¤¤à¤¨à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¨à¥‡à¤ªà¤¾à¤²à¥€ à¤­à¤¾à¤·à¤¾ à¤° à¤¸à¤¾à¤¹à¤¿à¤¤à¥à¤¯ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤—à¥Œà¤°à¤µ à¤¹à¥‹ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¸à¤²à¤¾à¤ˆ à¤¬à¤šà¤¾à¤à¤° à¤†à¤—à¤¾à¤®à¥€ à¤ªà¥à¤¸à¥à¤¤à¤¾à¤®à¤¾ à¤¹à¤¸à¥à¤¤à¤¾à¤¨à¥à¤¤à¤°à¤£ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "one.wav | Audio Duration: 11.50s | Transcription Time: 1.84s | WER: 0.158\n",
      "Prediction   : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤šà¥‹à¤˜à¤¾à¤°à¥à¤¨ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¥¤\n",
      "Ground Truth : à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤­à¤¾à¤·à¤¾ à¤¸à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿ à¤° à¤¸à¤‚à¤¸à¥à¤•à¤¾à¤° à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤ªà¤¹à¤¿à¤šà¤¾à¤¨ à¤¹à¥à¤¨à¥ à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¯à¤¿à¤¨à¤²à¤¾à¤ˆ à¤œà¤—à¥‡à¤°à¥à¤¨à¤¾ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¸à¤®à¤¾à¤œ à¤° à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¤²à¤¾à¤ˆ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "seven.wav | Audio Duration: 8.06s | Transcription Time: 1.23s | WER: 0.533\n",
      "Prediction   : à¤µà¤¾à¤¤à¤¾à¤µà¤¯à¤‚à¤• à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤ à¤¹à¥‹ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¥‡à¤¸à¤à¤— à¤®à¥‡à¤²à¤µà¤¿à¤¨à¤¾à¤¥ à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤ªà¤µà¤¿à¤·à¥à¤¯à¤°à¥ˆ à¤¸à¥à¤²à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "Ground Truth : à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤•à¥‹ à¤¸à¤‚à¤°à¤•à¥à¤·à¤£ à¤—à¤°à¥à¤¨à¥ à¤¹à¤¾à¤®à¥à¤°à¥‹ à¤¦à¤¾à¤¯à¤¿à¤¤à¥à¤µ à¤¹à¥‹à¥¤ à¤ªà¥à¤°à¤•à¥ƒà¤¤à¤¿à¤¸à¤à¤— à¤®à¥‡à¤²à¤®à¤¿à¤²à¤¾à¤ª à¤—à¤°à¥‡à¤° à¤®à¤¾à¤¤à¥à¤° à¤¹à¤¾à¤®à¥€ à¤­à¤µà¤¿à¤·à¥à¤¯à¤²à¤¾à¤ˆ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤¬à¤¨à¤¾à¤‰à¤¨ à¤¸à¤•à¥à¤›à¥Œà¤‚à¥¤\n",
      "\n",
      "six.wav | Audio Duration: 9.91s | Transcription Time: 1.47s | WER: 0.333\n",
      "Prediction   : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¯à¤¯ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥ à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¦à¥‡à¤¶à¤•à¥‹ à¤µà¤¿à¤•à¤¾à¤¸à¤®à¤¾ à¤¯à¥à¤µà¤¾ à¤µà¤°à¥à¤—à¤•à¥‹ à¤­à¥‚à¤®à¤¿à¤•à¤¾ à¤…à¤¤à¤¿à¤¨à¥ˆ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤› à¤¹à¤¾à¤®à¥€à¤²à¥‡ à¤¦à¥‡à¤¶à¤ªà¥à¤°à¤¤à¤¿ à¤œà¤¿à¤®à¥à¤®à¥‡à¤µà¤¾à¤°à¥€ à¤¬à¥‹à¤§ à¤—à¤°à¥‡à¤° à¤®à¥‡à¤¹à¤¨à¤¤ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "ten.wav | Audio Duration: 7.87s | Transcription Time: 1.14s | WER: 0.333\n",
      "Prediction   : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤µà¥ƒà¤°à¥à¤—à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯ à¤° à¤¶à¤¿à¤•à¥à¤·à¤¾à¤®à¤¾ à¤²à¤—à¤¾à¤¨à¥€ à¤—à¤°à¥à¤¨à¥ à¤®à¤¾à¤¤à¥à¤° à¤¦à¥€à¤°à¥à¤˜à¤•à¤¾à¤²à¥€à¤¨ à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹ à¤†à¤§à¤¾à¤° à¤¹à¥‹ à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤¾à¤—à¤°à¤¿à¤•à¤²à¥‡ à¤¯à¤¸à¤•à¥‹ à¤®à¤¹à¤¤à¥à¤µ à¤¬à¥à¤à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n",
      "three.wav | Audio Duration: 9.98s | Transcription Time: 1.48s | WER: 0.375\n",
      "Prediction   : à¤®à¥‡à¤¹à¥‡à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¤¿à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤›à¥¤ à¤•à¤¹à¤¿à¤²à¥‡ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤œà¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤˜à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "Ground Truth : à¤®à¥‡à¤¹à¤¨à¤¤ à¤° à¤‡à¤®à¤¾à¤¨à¥à¤¦à¤¾à¤°à¥€à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤°à¥ˆ à¤¸à¤«à¤²à¤¤à¤¾ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤—à¤°à¥à¤¨ à¤¸à¤•à¤¿à¤¨à¥à¤› à¤•à¤¹à¤¿à¤²à¥à¤¯à¥ˆ à¤¹à¤¾à¤° à¤¨à¤®à¤¾à¤¨à¥à¤¨à¥à¤¹à¥‹à¤¸à¥ à¤° à¤†à¤«à¥à¤¨à¥‹ à¤²à¤•à¥à¤·à¥à¤¯à¤®à¤¾ à¤…à¤—à¤¾à¤¡à¤¿ à¤¬à¤¢à¤¿à¤°à¤¹à¤¨à¥à¤¹à¥‹à¤¸à¥à¥¤\n",
      "\n",
      "two.wav | Audio Duration: 9.55s | Transcription Time: 1.48s | WER: 0.278\n",
      "Prediction   : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤à¥à¤¯à¤¾à¤²à¥‹à¤°à¤°à¥â€à¤¯à¤¾à¤‰à¤à¤›à¥¤ à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¥€ à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤® à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "Ground Truth : à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¸à¤¬à¥ˆà¤•à¥‹ à¤…à¤§à¤¿à¤•à¤¾à¤° à¤¹à¥‹ à¤° à¤¯à¤¸à¤²à¥‡ à¤®à¤¾à¤¤à¥à¤° à¤®à¤¾à¤¨à¤¿à¤¸à¤•à¥‹ à¤œà¥€à¤µà¤¨à¤®à¤¾ à¤‰à¤œà¥à¤¯à¤¾à¤²à¥‹ à¤²à¥à¤¯à¤¾à¤‰à¤à¤› à¤¹à¤¾à¤®à¥€ à¤¸à¤¬à¥ˆà¤²à¥‡ à¤¶à¤¿à¤•à¥à¤·à¤¾ à¤¹à¤¾à¤¸à¤¿à¤² à¤—à¤°à¥à¤¨ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›à¥¤\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_onnx_quantized = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_from_file(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_onnx_quantized.append({\n",
    "        \"model\": \"fine_tuned_models_quantized.onnx\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91d9fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_quantized(results_onnx_quantized):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_onnx_quantized:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Quantized ONNX Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "583ec230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quantized ONNX Model Summary ===\n",
      "Model: fine_tuned_models_quantized.onnx\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.48 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_quantized(results_onnx_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "901a57aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONNX Model Summary ===\n",
      "Model: onnx_FineTuned\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3819\n",
      "  Average Audio Length: 9.62 sec\n",
      "  Average Transcription Time: 1.57 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34d130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
