{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c70651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import time\n",
    "import os\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7b568",
   "metadata": {},
   "source": [
    "### Load and store model in the dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2bc961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and saving facebook/wav2vec2-large-960h-lv60-self...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "models = {\n",
    "\n",
    "    \"large_960h_lv60\": \"facebook/wav2vec2-large-960h-lv60-self\",\n",
    "\n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"Loading and saving {model_id}...\")\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_id,use_safetensors=True)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_id,use_safetensors=True)\n",
    "\n",
    "    # Create subfolder per model inside models folder\n",
    "    model_save_path = os.path.join(save_dir, name)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    # Save processor and model locally\n",
    "    processor.save_pretrained(model_save_path)\n",
    "    model.save_pretrained(model_save_path)\n",
    "\n",
    "print(\"All models loaded and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62cd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "models_dir = \"models\"\n",
    "\n",
    "test_data_dir = \"test_data\"\n",
    "transcribs_dir = \"ground_truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0730da6",
   "metadata": {},
   "source": [
    "### Load model from the dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a41cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [name for name in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, name))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e98c1",
   "metadata": {},
   "source": [
    "### load wav file and resample if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d274a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, target_sr = 16000):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc5f22",
   "metadata": {},
   "source": [
    "### Transcribe audio with given model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eed7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(waveform, processor, model):\n",
    "        # Ensure mono audio: get only one channel if multiple\n",
    "        if waveform.ndim == 2:\n",
    "            waveform = waveform[0]  # take the first channel\n",
    "        \n",
    "        # Ensure it's a tensor of shape (1, time)\n",
    "        input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "        return transcription.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95232741",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe441fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using model: large_960h_lv60 ---\n",
      "large_960h_lv60 | 1.wav | Audio Duration: 8.18s | Transcription Time: 7.00s | WER: 0.375\n",
      "Transcription: this is the test now there is a noise finished\n",
      "Ground Truth : this is test now there is noise finised\n",
      "\n",
      "large_960h_lv60 | 10.wav | Audio Duration: 7.75s | Transcription Time: 2.22s | WER: 0.222\n",
      "Transcription: tence design to be oven clearly each word is easy to pronounce and understand when rets aloud slowly\n",
      "Ground Truth : sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "large_960h_lv60 | 2.wav | Audio Duration: 14.72s | Transcription Time: 4.47s | WER: 0.842\n",
      "Transcription: sometimes life tis onas e if hi truns life antns not even i the e of confusionisas evrystep evente sloone e sentid ito soe stronger an misel\n",
      "Ground Truth : sometimes life does not go as planned we face struggles doubts and delays but even in the middle of confusion your path has purpose every step even the slow ones is shaping you into someone stronger and wiser\n",
      "\n",
      "large_960h_lv60 | 3.wav | Audio Duration: 32.62s | Transcription Time: 10.40s | WER: 0.780\n",
      "Transcription: my dear i neveto place an other for eliittis canni get a last tois tatis also as somo to a onlyon half of it just half fut e costlasiit tot cros it apalib o and i lik to oro one gae on te sid o ooty a eo o es akit a i iseo o oous rit my ereestiet tie apoeo i acar el tarits\n",
      "Ground Truth : hi there i would like to place an order for delivery please can i get a large pepperoni pizza with extra cheese also add some mushrooms and olives on half of it just half for the crust lets go with stuffed crust if thats available oh and i like to order one garlic bread on the side plus a 500 ml bottle of coke please make it a bit crispy but not overcooked just right my address is 22 king street apartment 4b i will pay by card when it arrives\n",
      "\n",
      "large_960h_lv60 | 4.wav | Audio Duration: 44.39s | Transcription Time: 14.75s | WER: 0.646\n",
      "Transcription: i am called intopo some tims to my car arly toda i was patly an somthing more wen anateecl cap tle sit of my compor it's not a mental accident but theteis some visible times scratches and thepopsen apelos ive taken fotuse from the at different angles and i also nitiste of the tibe la umber lotily know i a sot and pot micocot sti drive my car is asleepat twenty twenty two at a corlon the timis is mostley on the lef hear site i'd like to know the next sit an sould i bin the car in fon it pexo or carris in the forte first plese lame now ow to posit thank you\n",
      "Ground Truth : hi i am calling to report some damage to my car earlier today i was parked near a shopping mall when another vehicle scraped the rear side of my bumper it is not a major accident but there is some visible damage scratches and the bumper seems a bit loose i have taken photos from the different angles and i also noticed the other drivers plate number luckily no one was hurt and both vehicles could still drive my car is a silver 2020 toyota corolla the damage is mostly on the left rear side i would like to know the next steps should i bring the car in for inspection or can i send the photos first please let me know how to proceed thank you\n",
      "\n",
      "large_960h_lv60 | 5.wav | Audio Duration: 24.10s | Transcription Time: 7.48s | WER: 0.045\n",
      "Transcription: howt is the great way to practise pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "Ground Truth : loud is a great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n",
      "large_960h_lv60 | 6.wav | Audio Duration: 55.82s | Transcription Time: 18.99s | WER: 0.076\n",
      "Transcription: ing your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you don't need to speak fast in fact slower is easirly better just focus on making each word distinct and steady this full minute of speech is meant to help you taste transcription quality the more practice you get the better your clarity and fluency will be whether you're using vosq whisper or any other model the basic principle is the same speak clearly minimize background noise and be consistent you can read these passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you'll see great results in your voice recognizant experiments\n",
      "Ground Truth : your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "large_960h_lv60 | 7.wav | Audio Duration: 7.75s | Transcription Time: 2.12s | WER: 0.050\n",
      "Transcription: te simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "Ground Truth : a simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "large_960h_lv60 | 8.wav | Audio Duration: 52.87s | Transcription Time: 17.85s | WER: 0.392\n",
      "Transcription: is with tear sar sentences is important wen working with istis to tac models these models rely on gran adear and poper pronouncation to give etet results you dont latoispeak fast in fact slower is usually better just bocas on making is work distinct and stedy this full minute of i peece is made to help you tastran trancription quality the more pacticigat the better ocal clarity and fan eily whether you are using vorse whisper or any other model the basic principle is the same  speak clearly manimise better or nowis and be consistent you can red this pacis multipie thans ifnaret or even regard your own simplestories or experience the key is to isterelix and natural good or dealie to better transcription keep going and you will see great results in your voce regagins and recognison experiments\n",
      "Ground Truth : speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "large_960h_lv60 | 9.wav | Audio Duration: 23.95s | Transcription Time: 7.48s | WER: 0.175\n",
      "Transcription: great way to practice pronunciation speaking clearly helps interpe understanding and communication this sort paragraph is filled with simple easy words you can repeat it multipied times to be to get better whisper and other spece modern models perform best when the adio is clean the the pace is steady and the pronunciation is consistent take your time while reading and try not to rust the word\n",
      "Ground Truth : great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n--- Using model: {model_name} ---\")\n",
    "    model_path = os.path.join(models_dir, model_name)\n",
    "    \n",
    "    # Load processor and model\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    # Loop through all wav files\n",
    "    for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "        if not wav_file.endswith(\".wav\"):\n",
    "            continue\n",
    "        \n",
    "        wav_path = os.path.join(test_data_dir, wav_file)\n",
    "        txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "        txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "        if not os.path.isfile(txt_path):\n",
    "            print(f\"Transcription file missing for {wav_file}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Load audio to get duration\n",
    "        speech_array, sample_rate = sf.read(wav_path)\n",
    "        audio_duration = len(speech_array) / sample_rate\n",
    "        \n",
    "        # Load audio waveform for model input\n",
    "        waveform = load_audio(wav_path)  # Your existing function\n",
    "        \n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ground_truth = f.read().strip().lower()\n",
    "\n",
    "        # Transcribe & measure time\n",
    "        start_time = time.time()\n",
    "        pred_text = transcribe(waveform, processor, model)\n",
    "        transcription_time = time.time() - start_time\n",
    "\n",
    "        # Calculate WER\n",
    "        error_rate = wer(ground_truth, pred_text)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{model_name} | {wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "        print(f\"Transcription: {pred_text}\")\n",
    "        print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "        # Store result including audio duration and transcription time\n",
    "        results.append({\n",
    "            \"model\": model_name,\n",
    "            \"audio_file\": wav_file,\n",
    "            \"audio_duration_sec\": round(audio_duration, 2),\n",
    "            \"transcription_time_sec\": round(transcription_time, 2),\n",
    "            \"wer\": error_rate,\n",
    "            \"prediction\": pred_text,\n",
    "            \"ground_truth\": ground_truth\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6411f252",
   "metadata": {},
   "source": [
    "### Average Error per word of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d537a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_wer(results):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_stats = defaultdict(lambda: {\n",
    "        \"total_wer\": 0.0,\n",
    "        \"total_audio_duration\": 0.0,\n",
    "        \"total_transcription_time\": 0.0,\n",
    "        \"count\": 0\n",
    "    })\n",
    "\n",
    "    for result in results:\n",
    "        model = result[\"model\"]\n",
    "        model_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "        model_stats[model][\"count\"] += 1\n",
    "\n",
    "    print(\"\\n=== Average Metrics per Model ===\")\n",
    "    for model, stats in model_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_duration = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_transcription_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files        : {count}\")\n",
    "        print(f\"  Average WER            : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Duration : {avg_audio_duration:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time : {avg_transcription_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b389c944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics per Model ===\n",
      "Model: large_960h_lv60\n",
      "  Number of files        : 10\n",
      "  Average WER            : 0.3603\n",
      "  Average Audio Duration : 27.21 sec\n",
      "  Average Transcription Time : 9.28 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24374d",
   "metadata": {},
   "source": [
    "## Export large model into ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641d638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = \"models/large_960h_lv60/\"\n",
    "model_onx = \"models/large_960h_lv60/\"\n",
    "onx_model = \"large_960h_lv60.onnx\"\n",
    "test_data = \"test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a93cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(model_onx, onx_model):\n",
    "    print(f\"Converting mode {model_onx}\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_onx)\n",
    "    model.eval()\n",
    "\n",
    "    audio_len = 16000\n",
    "    dummy_input = torch.randn(1, audio_len)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,     \n",
    "        dummy_input,   \n",
    "        onx_model,             \n",
    "        export_params=True,     \n",
    "        opset_version=14,         \n",
    "        do_constant_folding=True,    \n",
    "        input_names=[\"input_values\"],\n",
    "        output_names=[\"logits\"],     \n",
    "        dynamic_axes={\n",
    "            \"input_values\": {1: \"sequence_length\"},\n",
    "            \"logits\": {1: \"sequence_length\"}\n",
    "        },                          # allow variable-length audio sequences\n",
    "    )\n",
    "    print(f\"ONNX model saved at: {onx_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6b85df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting mode models/large_960h_lv60/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:74: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved at: large_960h_lv60.onnx\n"
     ]
    }
   ],
   "source": [
    "convert_model_to_onnx(model_onx, onx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5701a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_onx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80b869",
   "metadata": {},
   "source": [
    "\n",
    "### Load ONNX model with optimized session (model less than 2GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08db537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = rt.InferenceSession(onx_model, sess_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704322f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file(filepath):\n",
    "    # Load audio\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    # If not mono, convert to mono\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "        sr = 16000\n",
    "\n",
    "    # Prepare input for ONNX\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.numpy()\n",
    "\n",
    "    # Run ONNX inference\n",
    "    ort_inputs = {session.get_inputs()[0].name: input_values}\n",
    "    ort_outputs = session.run(None, ort_inputs)[0]\n",
    "\n",
    "    # Decode prediction\n",
    "    predicted_ids = np.argmax(ort_outputs, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bdc9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.wav | Audio Duration: 8.18s | Transcription Time: 2.10s | WER: 0.375\n",
      "Prediction   : this is the test now there is a noise finished\n",
      "Ground Truth : this is test now there is noise finised\n",
      "\n",
      "10.wav | Audio Duration: 7.75s | Transcription Time: 2.00s | WER: 0.222\n",
      "Prediction   : tence design to be oven clearly each word is easy to pronounce and understand when rets aloud slowly\n",
      "Ground Truth : sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "2.wav | Audio Duration: 14.72s | Transcription Time: 3.89s | WER: 0.711\n",
      "Prediction   : sometimes life te sontis tine if hi true life snts not eveny teien of confusion  isas every step even the stoon sepingnito so stronger and wiser\n",
      "Ground Truth : sometimes life does not go as planned we face struggles doubts and delays but even in the middle of confusion your path has purpose every step even the slow ones is shaping you into someone stronger and wiser\n",
      "\n",
      "3.wav | Audio Duration: 32.62s | Transcription Time: 9.62s | WER: 0.769\n",
      "Prediction   : my dear iio lace an oter oelatis canni get a last otiaaacis also a son ma a onlio half of it just ha fit the cosilis oit to s it as palib on and i like to or o one gae on tesid o oity aio o please make it a bg criset o oous right my adreses tee in street apoi oe i'll a a cari arits\n",
      "Ground Truth : hi there i would like to place an order for delivery please can i get a large pepperoni pizza with extra cheese also add some mushrooms and olives on half of it just half for the crust lets go with stuffed crust if thats available oh and i like to order one garlic bread on the side plus a 500 ml bottle of coke please make it a bit crispy but not overcooked just right my address is 22 king street apartment 4b i will pay by card when it arrives\n",
      "\n",
      "4.wav | Audio Duration: 44.39s | Transcription Time: 14.49s | WER: 0.606\n",
      "Prediction   : i am callein to po sone tim to my car orly to da i a paly an soting more en anitetacl crap tele sight of my poper it's not a mental accident but theteis some visible timos scratches and teporpsen epetlos i've taken focus from the at different angles and i also nintte of the tive lat numbr lortily know i aot and pot micocot still drive my car is a slipat twenty twenty two at a corler the ti missis mostley on he lef ear side i'd like to know the next tat an should i bin the car in fon it pexton or can i send the foto first pleas la me know how to procet think you\n",
      "Ground Truth : hi i am calling to report some damage to my car earlier today i was parked near a shopping mall when another vehicle scraped the rear side of my bumper it is not a major accident but there is some visible damage scratches and the bumper seems a bit loose i have taken photos from the different angles and i also noticed the other drivers plate number luckily no one was hurt and both vehicles could still drive my car is a silver 2020 toyota corolla the damage is mostly on the left rear side i would like to know the next steps should i bring the car in for inspection or can i send the photos first please let me know how to proceed thank you\n",
      "\n",
      "5.wav | Audio Duration: 24.10s | Transcription Time: 6.48s | WER: 0.045\n",
      "Prediction   : howt is the great way to practise pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "Ground Truth : loud is a great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n",
      "6.wav | Audio Duration: 55.82s | Transcription Time: 20.82s | WER: 0.076\n",
      "Prediction   : ing your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you don't need to speak fast in fact slower is easirly better just focus on making each word distinct and steady this full minute of speech is meant to help you taste transcription quality the more practice you get the better your clarity and fluency will be whether you're using vosq whisper or any other model the basic principle is the same speak clearly minimize background noise and be consistent you can read these passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you'll see great results in your voice recognizant experiments\n",
      "Ground Truth : your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "7.wav | Audio Duration: 7.75s | Transcription Time: 1.94s | WER: 0.050\n",
      "Prediction   : te simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "Ground Truth : a simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "8.wav | Audio Duration: 52.87s | Transcription Time: 30.42s | WER: 0.392\n",
      "Prediction   : is with tear sar sentences is important wen working with istis to tac models these models rely on gran adear and poper pronouncation to give etet results you dont latoispeak fast in fact slower is usually better just bocas on making is work distinct and stedy this full minute of i peece is made to help you tastran trancription quality the more pacticigat the better ocal clarity and fan eily whether you are using vorse whisper or any other model the basic principle is the same  speak clearly manimise better or nowis and be consistent you can red this pacis multipie thans ifnaret or even regard your own simplestories or experience the key is to isterelix and natural good or dealie to better transcription keep going and you will see great results in your voce regagins and recognison experiments\n",
      "Ground Truth : speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "9.wav | Audio Duration: 23.95s | Transcription Time: 17.17s | WER: 0.175\n",
      "Prediction   : great way to practice pronunciation speaking clearly helps interpe understanding and communication this sort paragraph is filled with simple easy words you can repeat it multipied times to be to get better whisper and other spece modern models perform best when the adio is clean the the pace is steady and the pronunciation is consistent take your time while reading and try not to rust the word\n",
      "Ground Truth : great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_onnx = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_from_file(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_onnx.append({\n",
    "        \"model\": \"onnx_large_960h_lv60\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4638f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_o(results_onnx):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_onnx:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== ONNX Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2aa8e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONNX Model Summary ===\n",
      "Model: onnx_large_960h_lv60\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3421\n",
      "  Average Audio Length: 27.21 sec\n",
      "  Average Transcription Time: 10.89 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8488831",
   "metadata": {},
   "source": [
    "### Converting into OpenVino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b25f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a89c0d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "example = torch.randn(1, 160000)\n",
    "open_vino_model = ov.convert_model(model, example_input = (example,))\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1e923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model  = core.compile_model(open_vino_model, 'CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e18badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_from_file_ov(filepath):\n",
    "    speech_array, sr = sf.read(filepath)\n",
    "\n",
    "    if len(speech_array.shape) > 1:\n",
    "        speech_array = np.mean(speech_array, axis=1)\n",
    "\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array).float()).numpy()\n",
    "\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"np\")\n",
    "    input_values = inputs[\"input_values\"]\n",
    "\n",
    "    ov_inputs = {0: input_values}\n",
    "    ov_outputs = compiled_model(ov_inputs)\n",
    "    logits = list(ov_outputs.values())[0]\n",
    "\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d9c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.wav | Audio Duration: 8.18s | Transcription Time: 1.35s | WER: 0.375\n",
      "🔊 Prediction   : this is the test now there is a noise finished\n",
      "📜 Ground Truth : this is test now there is noise finised\n",
      "\n",
      "10.wav | Audio Duration: 7.75s | Transcription Time: 0.98s | WER: 0.222\n",
      "🔊 Prediction   : tence design to be oven clearly each word is easy to pronounce and understand when rets aloud slowly\n",
      "📜 Ground Truth : sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "2.wav | Audio Duration: 14.72s | Transcription Time: 1.75s | WER: 0.711\n",
      "🔊 Prediction   : sometimes life te sontis tine if hi true life snts not eveny teien of confusion  isas every step even the stoon sepingnito so stronger and wiser\n",
      "📜 Ground Truth : sometimes life does not go as planned we face struggles doubts and delays but even in the middle of confusion your path has purpose every step even the slow ones is shaping you into someone stronger and wiser\n",
      "\n",
      "3.wav | Audio Duration: 32.62s | Transcription Time: 4.50s | WER: 0.769\n",
      "🔊 Prediction   : my dear iio lace an oter oelatis canni get a last otiaaacis also a son ma a onlio half of it just ha fit the cosilis oit to s it as palib on and i like to or o one gae on tesid o oity aio o please make it a bg criset o oous right my adreses tee in street apoi oe i'll a a cari arits\n",
      "📜 Ground Truth : hi there i would like to place an order for delivery please can i get a large pepperoni pizza with extra cheese also add some mushrooms and olives on half of it just half for the crust lets go with stuffed crust if thats available oh and i like to order one garlic bread on the side plus a 500 ml bottle of coke please make it a bit crispy but not overcooked just right my address is 22 king street apartment 4b i will pay by card when it arrives\n",
      "\n",
      "4.wav | Audio Duration: 44.39s | Transcription Time: 8.60s | WER: 0.606\n",
      "🔊 Prediction   : i am callein to po sone tim to my car orly to da i a paly an soting more en anitetacl crap tele sight of my poper it's not a mental accident but theteis some visible timos scratches and teporpsen epetlos i've taken focus from the at different angles and i also nintte of the tive lat numbr lortily know i aot and pot micocot still drive my car is a slipat twenty twenty two at a corler the ti missis mostley on he lef ear side i'd like to know the next tat an should i bin the car in fon it pexton or can i send the foto first pleas la me know how to procet think you\n",
      "📜 Ground Truth : hi i am calling to report some damage to my car earlier today i was parked near a shopping mall when another vehicle scraped the rear side of my bumper it is not a major accident but there is some visible damage scratches and the bumper seems a bit loose i have taken photos from the different angles and i also noticed the other drivers plate number luckily no one was hurt and both vehicles could still drive my car is a silver 2020 toyota corolla the damage is mostly on the left rear side i would like to know the next steps should i bring the car in for inspection or can i send the photos first please let me know how to proceed thank you\n",
      "\n",
      "5.wav | Audio Duration: 24.10s | Transcription Time: 3.05s | WER: 0.045\n",
      "🔊 Prediction   : howt is the great way to practise pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "📜 Ground Truth : loud is a great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n",
      "6.wav | Audio Duration: 55.82s | Transcription Time: 9.75s | WER: 0.076\n",
      "🔊 Prediction   : ing your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you don't need to speak fast in fact slower is easirly better just focus on making each word distinct and steady this full minute of speech is meant to help you taste transcription quality the more practice you get the better your clarity and fluency will be whether you're using vosq whisper or any other model the basic principle is the same speak clearly minimize background noise and be consistent you can read these passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you'll see great results in your voice recognizant experiments\n",
      "📜 Ground Truth : your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "7.wav | Audio Duration: 7.75s | Transcription Time: 0.74s | WER: 0.050\n",
      "🔊 Prediction   : te simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "📜 Ground Truth : a simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "8.wav | Audio Duration: 52.87s | Transcription Time: 8.56s | WER: 0.392\n",
      "🔊 Prediction   : is with tear sar sentences is important wen working with istis to tac models these models rely on gran adear and poper pronouncation to give etet results you dont latoispeak fast in fact slower is usually better just bocas on making is work distinct and stedy this full minute of i peece is made to help you tastran trancription quality the more pacticigat the better ocal clarity and fan eily whether you are using vorse whisper or any other model the basic principle is the same  speak clearly manimise better or nowis and be consistent you can red this pacis multipie thans ifnaret or even regard your own simplestories or experience the key is to isterelix and natural good or dealie to better transcription keep going and you will see great results in your voce regagins and recognison experiments\n",
      "📜 Ground Truth : speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "9.wav | Audio Duration: 23.95s | Transcription Time: 2.97s | WER: 0.175\n",
      "🔊 Prediction   : great way to practice pronunciation speaking clearly helps interpe understanding and communication this sort paragraph is filled with simple easy words you can repeat it multipied times to be to get better whisper and other spece modern models perform best when the adio is clean the the pace is steady and the pronunciation is consistent take your time while reading and try not to rust the word\n",
      "📜 Ground Truth : great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "results_openvino = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"❌ Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        prediction = transcribe_from_file_ov(wav_path)\n",
    "        transcription_time = time.time() - start_time\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"🔊 Prediction   : {prediction}\")\n",
    "    print(f\"📜 Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    results_openvino.append({\n",
    "        \"model\": \"openvino_large_960h_lv60\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2f601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b772a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer(results_openvino):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_openvino:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Openvino Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2a8527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Openvino Model Summary ===\n",
      "Model: openvino_large_960h_lv60\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3421\n",
      "  Average Audio Length: 27.21 sec\n",
      "  Average Transcription Time: 4.22 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer(results_openvino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eed7f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONNX Model Summary ===\n",
      "Model: onnx_large_960h_lv60\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3421\n",
      "  Average Audio Length: 27.21 sec\n",
      "  Average Transcription Time: 10.89 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aafbb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics per Model ===\n",
      "Model: large_960h_lv60\n",
      "  Number of files        : 10\n",
      "  Average WER            : 0.3603\n",
      "  Average Audio Duration : 27.21 sec\n",
      "  Average Transcription Time : 9.28 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf2407",
   "metadata": {},
   "source": [
    "### Checking ONNX model size (should be less than 2GB to optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3388a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1262392509 bytes\n",
      "Model size: 1203.91 MB\n",
      "Model size: 1.18 GB\n",
      "Model size is LESS than or equal to 2GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"large_960h_lv60.onnx\"\n",
    "size_bytes = os.path.getsize(model_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_mb / 1024\n",
    "\n",
    "print(f\"Model size: {size_bytes} bytes\")\n",
    "print(f\"Model size: {size_mb:.2f} MB\")\n",
    "print(f\"Model size: {size_gb:.2f} GB\")\n",
    "\n",
    "if size_gb > 2:\n",
    "    print(\"Model size is GREATER than 2GB\")\n",
    "else:\n",
    "    print(\"Model size is LESS than or equal to 2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22372557",
   "metadata": {},
   "source": [
    "### Dynamic Quantizing large960h lv60 pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61eccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce725667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32fa82",
   "metadata": {},
   "source": [
    "### Dynamic quantizing the linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb3a563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57a6f6",
   "metadata": {},
   "source": [
    "### save the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "015966eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model state_dict saved to: models\\dynamic_quantized_model_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "quantized_path = os.path.join(models_dir, \"dynamic_quantized_model_state_dict.pth\")\n",
    "torch.save(dynamic_quantized_model.state_dict(), quantized_path)\n",
    "print(f\"Quantized model state_dict saved to: {quantized_path}\")\n",
    "#to load later|\n",
    "#quantized_model.load_state_dict(torch.load(quantized_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54a412",
   "metadata": {},
   "source": [
    "### Dynamic quantizing ONNX model (large960h-lv60 model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58139591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model_path = \"large_960h_lv60.onnx\"\n",
    "model = onnx.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccac9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef3234",
   "metadata": {},
   "source": [
    "#### Pre-processing model before quantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e319bf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message with type 'onnx.ModelProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDecodeError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mquant_pre_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlarge_960h_lv60.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlarge_960h_lv60_optimized.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_as_external_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_tensors_to_one_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexternal_data_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlarge_960h_lv60.data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnxruntime\\quantization\\shape_inference.py:79\u001b[39m, in \u001b[36mquant_pre_process\u001b[39m\u001b[34m(input_model, output_model_path, skip_optimization, skip_onnx_shape, skip_symbolic_shape, auto_merge, int_max, guess_output_rank, verbose, save_as_external_data, all_tensors_to_one_file, external_data_location, external_data_size_threshold, **deprecated_kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_symbolic_shape:\n\u001b[32m     78\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mPerforming symbolic shape inference...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     loaded_model = input_model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_model, onnx.ModelProto) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     model = SymbolicShapeInference.infer_shapes(\n\u001b[32m     81\u001b[39m         loaded_model,\n\u001b[32m     82\u001b[39m         int_max,\n\u001b[32m   (...)\u001b[39m\u001b[32m     85\u001b[39m         verbose,\n\u001b[32m     86\u001b[39m     )\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_optimization:\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# Use ORT optimizers (native code) to optimize model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnx\\__init__.py:226\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(f, format, load_external_data)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\n\u001b[32m    206\u001b[39m     f: IO[\u001b[38;5;28mbytes\u001b[39m] | \u001b[38;5;28mstr\u001b[39m | os.PathLike,\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mformat\u001b[39m: _SupportedFormat | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: A002\u001b[39;00m\n\u001b[32m    208\u001b[39m     load_external_data: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    209\u001b[39m ) -> ModelProto:\n\u001b[32m    210\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Loads a serialized ModelProto into memory.\u001b[39;00m\n\u001b[32m    211\u001b[39m \n\u001b[32m    212\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m \u001b[33;03m        Loaded in-memory ModelProto.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     model = \u001b[43m_get_serializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_load_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelProto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_external_data:\n\u001b[32m    229\u001b[39m         model_filepath = _get_file_path(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnx\\serialization.py:121\u001b[39m, in \u001b[36m_ProtobufSerializer.deserialize_proto\u001b[39m\u001b[34m(self, serialized, proto)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(serialized, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    119\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParameter \u001b[39m\u001b[33m'\u001b[39m\u001b[33mserialized\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be bytes, but got type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(serialized)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m decoded = typing.cast(Optional[\u001b[38;5;28mint\u001b[39m], \u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParseFromString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoded \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoded != \u001b[38;5;28mlen\u001b[39m(serialized):\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m google.protobuf.message.DecodeError(\n\u001b[32m    124\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProtobuf decoding consumed too few bytes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(serialized)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[31mDecodeError\u001b[39m: Error parsing message with type 'onnx.ModelProto'"
     ]
    }
   ],
   "source": [
    "quant_pre_process(\n",
    "    input_model=\"large_960h_lv60.onnx\",\n",
    "    output_model_path=\"large_960h_lv60_optimized.onnx\",\n",
    "    save_as_external_data=True,\n",
    "    all_tensors_to_one_file=True,               \n",
    "    external_data_location=\"large_960h_lv60.data\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20051517",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlarge_960h_lv60_optimized.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlarge_960h_lv60_quantized.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQuantType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQInt8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnxruntime\\quantization\\quantize.py:867\u001b[39m, in \u001b[36mquantize_dynamic\u001b[39m\u001b[34m(model_input, model_output, op_types_to_quantize, per_channel, reduce_range, weight_type, nodes_to_quantize, nodes_to_exclude, use_external_data_format, extra_options)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMatMulConstBOnly\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m extra_options:\n\u001b[32m    865\u001b[39m     extra_options[\u001b[33m\"\u001b[39m\u001b[33mMatMulConstBOnly\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m quantizer = \u001b[43mONNXQuantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduce_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# static\u001b[39;49;00m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mQuantType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQUInt8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# dynamic activation only supports uint8\u001b[39;49;00m\n\u001b[32m    875\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes_to_quantize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes_to_exclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_types_to_quantize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    882\u001b[39m quantizer.quantize_model()\n\u001b[32m    883\u001b[39m quantizer.model.save_model_to_file(model_output, use_external_data_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnxruntime\\quantization\\onnx_quantizer.py:70\u001b[39m, in \u001b[36mONNXQuantizer.__init__\u001b[39m\u001b[34m(self, model, per_channel, reduce_range, mode, static, weight_qType, activation_qType, tensors_range, nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, extra_options)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m.model.replace_gemm_with_matmul()\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# We need to update value_infos.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m model = \u001b[43msave_and_reload_model_with_shape_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m.value_infos = {vi.name: vi \u001b[38;5;28;01mfor\u001b[39;00m vi \u001b[38;5;129;01min\u001b[39;00m model.graph.value_info}\n\u001b[32m     72\u001b[39m \u001b[38;5;28mself\u001b[39m.value_infos.update({ot.name: ot \u001b[38;5;28;01mfor\u001b[39;00m ot \u001b[38;5;129;01min\u001b[39;00m model.graph.output})\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnxruntime\\quantization\\quant_utils.py:988\u001b[39m, in \u001b[36msave_and_reload_model_with_shape_infer\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    986\u001b[39m model_path = Path(quant_tmp_dir).joinpath(\u001b[33m\"\u001b[39m\u001b[33mmodel.onnx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    987\u001b[39m onnx.save_model(model_copy, model_path.as_posix(), save_as_external_data=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_with_shape_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnxruntime\\quantization\\quant_utils.py:977\u001b[39m, in \u001b[36mload_model_with_shape_infer\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m    975\u001b[39m inferred_model_path = generate_identified_filename(model_path, \u001b[33m\"\u001b[39m\u001b[33m-inferred\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    976\u001b[39m onnx.shape_inference.infer_shapes_path(\u001b[38;5;28mstr\u001b[39m(model_path), \u001b[38;5;28mstr\u001b[39m(inferred_model_path))\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m model = \u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minferred_model_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    978\u001b[39m add_infer_metadata(model)\n\u001b[32m    979\u001b[39m inferred_model_path.unlink()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnx\\__init__.py:232\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(f, format, load_external_data)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_filepath:\n\u001b[32m    231\u001b[39m         base_dir = os.path.dirname(model_filepath)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m         \u001b[43mload_external_data_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnx\\external_data_helper.py:75\u001b[39m, in \u001b[36mload_external_data_for_model\u001b[39m\u001b[34m(model, base_dir)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m _get_all_tensors(model):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m uses_external_data(tensor):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[43mload_external_data_for_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m         \u001b[38;5;66;03m# After loading raw_data from external_data, change the state of tensors\u001b[39;00m\n\u001b[32m     77\u001b[39m         tensor.data_location = TensorProto.DEFAULT\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\onnx\\external_data_helper.py:61\u001b[39m, in \u001b[36mload_external_data_for_tensor\u001b[39m\u001b[34m(tensor, base_dir)\u001b[39m\n\u001b[32m     58\u001b[39m     data_file.seek(info.offset)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info.length:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     tensor.raw_data = \u001b[43mdata_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     63\u001b[39m     tensor.raw_data = data_file.read()\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "quantize_dynamic(\n",
    "    model_input=\"large_960h_lv60_optimized.onnx\",  \n",
    "    model_output=\"large_960h_lv60_quantized.onnx\", \n",
    "    weight_type=QuantType.QInt8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ed6bc",
   "metadata": {},
   "source": [
    "### Transcribe audio using quantized pytorch model large 960h lv60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c343b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Wav2Vec2_Hridaya\\venv\\Lib\\site-packages\\torch\\_utils.py:425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(quantized_path)\n",
    "dynamic_quantized_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "842cabcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "341c54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_with_quantized_model(wav_path):\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(wav_path)\n",
    "\n",
    "    # Convert stereo to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Remove batch/channel dimension\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = dynamic_quantized_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9ab5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.wav | Audio Duration: 8.18s | Transcription Time: 1.32s | WER: 0.375\n",
      "Prediction   : this is the test now there is a noise finished\n",
      "Ground Truth : this is test now there is noise finised\n",
      "\n",
      "10.wav | Audio Duration: 7.75s | Transcription Time: 0.84s | WER: 0.222\n",
      "Prediction   : tene design to be oven clearly each word is easy to pronounce and understand when its aloud slowly\n",
      "Ground Truth : sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "2.wav | Audio Duration: 14.72s | Transcription Time: 1.84s | WER: 0.895\n",
      "Prediction   : sometimes life disots i if his to life nas not eveni teiin of confusionasas evrystep even te oo  sapintoa stonganise\n",
      "Ground Truth : sometimes life does not go as planned we face struggles doubts and delays but even in the middle of confusion your path has purpose every step even the slow ones is shaping you into someone stronger and wiser\n",
      "\n",
      "3.wav | Audio Duration: 32.62s | Transcription Time: 4.97s | WER: 0.857\n",
      "Prediction   : ydear ineolace an oter olatis canniget a las oieseoes also as so o oa onlio havejust a fot he cosltto s it aa on and i like to or o one canie on tesido otyao o please make it a biise o o oou ri my evrysieeten stee aoeo a eecare aris\n",
      "Ground Truth : hi there i would like to place an order for delivery please can i get a large pepperoni pizza with extra cheese also add some mushrooms and olives on half of it just half for the crust lets go with stuffed crust if thats available oh and i like to order one garlic bread on the side plus a 500 ml bottle of coke please make it a bit crispy but not overcooked just right my address is 22 king street apartment 4b i will pay by card when it arrives\n",
      "\n",
      "4.wav | Audio Duration: 44.39s | Transcription Time: 7.11s | WER: 0.661\n",
      "Prediction   : i am called to o sone tims to ma car orlye today iwa paly an sothing more wen anote ecal crap thee site of mi ope it's not a mental accident but thits some visible times scratches and teporpsen opetlos i've taken focus from the at different angles and i also ninteto of the divelat umber lortili now iao and pot micolcot stil drive my car is a slipat twenty twenty two at a corler the tim misis mosley on e lef eor side id like to know the next tat an should i in the car in fon itpeson orcanis in the forto first ples late me now how to poset think you\n",
      "Ground Truth : hi i am calling to report some damage to my car earlier today i was parked near a shopping mall when another vehicle scraped the rear side of my bumper it is not a major accident but there is some visible damage scratches and the bumper seems a bit loose i have taken photos from the different angles and i also noticed the other drivers plate number luckily no one was hurt and both vehicles could still drive my car is a silver 2020 toyota corolla the damage is mostly on the left rear side i would like to know the next steps should i bring the car in for inspection or can i send the photos first please let me know how to proceed thank you\n",
      "\n",
      "5.wav | Audio Duration: 24.10s | Transcription Time: 3.69s | WER: 0.076\n",
      "Prediction   : ow t is the great way to practise pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the ardio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "Ground Truth : loud is a great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n",
      "6.wav | Audio Duration: 55.82s | Transcription Time: 8.84s | WER: 0.097\n",
      "Prediction   : ting your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you don't need to speak fast in fact slower is easerly better just focus on making each word distinct and steady this full minute of speech is meant to help you taste transitent quality the more practice you get the better your clarity and fluency will be whether youare using vosq whisper or any other model the basic principle is the same speak clearly minimize bacgrod noise and be consistent you can read these passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcition keep going and you'll see great results in your voice recognizant experiments\n",
      "Ground Truth : your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "7.wav | Audio Duration: 7.75s | Transcription Time: 0.83s | WER: 0.000\n",
      "Prediction   : a simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "Ground Truth : a simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "8.wav | Audio Duration: 52.87s | Transcription Time: 7.78s | WER: 0.490\n",
      "Prediction   : is with tear sart entenceis is important wen working with iste to tace models these models rely on gran ader and sofer pronouncation to give eet results your ton lyto istec pars in fact slower is usally better just focu on making is work distinct andstedy this full minute of i pece is made to helt you tas tran tracripsion quality the more practicige the better  yourcal clarity and fans yoi y wether you are using vore whisper or any other model the basic principle is the same e speak clearly manymase betteror nois and be consistent youcanret thes pec is multipile tans if mered or even regar your own simpoleistories or experience the key is to isterelix and natural good ordeolice to better transcripsion keep going and you will seegreat results in your voce recadis and reconisen experiments\n",
      "Ground Truth : speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "9.wav | Audio Duration: 23.95s | Transcription Time: 2.92s | WER: 0.222\n",
      "Prediction   : great way to practice pronunciation speaking clearly helps introve understanding and communication this sort paragraph is filled with simple easy words you can repeat it multiplied times to be to get better whisper and other spece modern models perfon best when the adio is clean the the pace issteady and the pronunciation is consistent take your time while reading and try not to rust the word\n",
      "Ground Truth : great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_quantized = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_with_quantized_model(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_quantized.append({\n",
    "        \"model\": dynamic_quantized_model,\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "886a2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model_wer_quantized(results_quantized):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_stats = defaultdict(lambda: {\n",
    "        \"total_wer\": 0.0,\n",
    "        \"total_audio_duration\": 0.0,\n",
    "        \"total_transcription_time\": 0.0,\n",
    "        \"count\": 0\n",
    "    })\n",
    "\n",
    "    for result in results_quantized:\n",
    "        model = result[\"model\"]\n",
    "        model_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "        model_stats[model][\"count\"] += 1\n",
    "\n",
    "    print(\"\\n=== Average Metrics per Model ===\")\n",
    "    for model, stats in model_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_duration = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_transcription_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files        : {count}\")\n",
    "        print(f\"  Average WER            : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Duration : {avg_audio_duration:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time : {avg_transcription_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "310c9d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics per Model ===\n",
      "Model: Wav2Vec2ForCTC(\n",
      "  (wav2vec2): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): DynamicQuantizedLinear(in_features=512, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "          (attention): Wav2Vec2Attention(\n",
      "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      ")\n",
      "  Number of files        : 10\n",
      "  Average WER            : 0.3895\n",
      "  Average Audio Duration : 27.21 sec\n",
      "  Average Transcription Time : 4.01 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer_quantized(results_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13e21a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics per Model ===\n",
      "Model: large_960h_lv60\n",
      "  Number of files        : 10\n",
      "  Average WER            : 0.3603\n",
      "  Average Audio Duration : 27.21 sec\n",
      "  Average Transcription Time : 9.28 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model_wer(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5e4ab",
   "metadata": {},
   "source": [
    "### transcribe audio using quantized ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40fb6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.wav | Audio Duration: 8.18s | Transcription Time: 1.64s | WER: 0.375\n",
      "Prediction   : this is the test now there is a noise finished\n",
      "Ground Truth : this is test now there is noise finised\n",
      "\n",
      "10.wav | Audio Duration: 7.75s | Transcription Time: 1.30s | WER: 0.222\n",
      "Prediction   : tence design to be oven clearly each word is easy to pronounce and understand when rets aloud slowly\n",
      "Ground Truth : sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "2.wav | Audio Duration: 14.72s | Transcription Time: 3.03s | WER: 0.711\n",
      "Prediction   : sometimes life te sontis tine if hi true life snts not eveny teien of confusion  isas every step even the stoon sepingnito so stronger and wiser\n",
      "Ground Truth : sometimes life does not go as planned we face struggles doubts and delays but even in the middle of confusion your path has purpose every step even the slow ones is shaping you into someone stronger and wiser\n",
      "\n",
      "3.wav | Audio Duration: 32.62s | Transcription Time: 8.49s | WER: 0.769\n",
      "Prediction   : my dear iio lace an oter oelatis canni get a last otiaaacis also a son ma a onlio half of it just ha fit the cosilis oit to s it as palib on and i like to or o one gae on tesid o oity aio o please make it a bg criset o oous right my adreses tee in street apoi oe i'll a a cari arits\n",
      "Ground Truth : hi there i would like to place an order for delivery please can i get a large pepperoni pizza with extra cheese also add some mushrooms and olives on half of it just half for the crust lets go with stuffed crust if thats available oh and i like to order one garlic bread on the side plus a 500 ml bottle of coke please make it a bit crispy but not overcooked just right my address is 22 king street apartment 4b i will pay by card when it arrives\n",
      "\n",
      "4.wav | Audio Duration: 44.39s | Transcription Time: 14.33s | WER: 0.606\n",
      "Prediction   : i am callein to po sone tim to my car orly to da i a paly an soting more en anitetacl crap tele sight of my poper it's not a mental accident but theteis some visible timos scratches and teporpsen epetlos i've taken focus from the at different angles and i also nintte of the tive lat numbr lortily know i aot and pot micocot still drive my car is a slipat twenty twenty two at a corler the ti missis mostley on he lef ear side i'd like to know the next tat an should i bin the car in fon it pexton or can i send the foto first pleas la me know how to procet think you\n",
      "Ground Truth : hi i am calling to report some damage to my car earlier today i was parked near a shopping mall when another vehicle scraped the rear side of my bumper it is not a major accident but there is some visible damage scratches and the bumper seems a bit loose i have taken photos from the different angles and i also noticed the other drivers plate number luckily no one was hurt and both vehicles could still drive my car is a silver 2020 toyota corolla the damage is mostly on the left rear side i would like to know the next steps should i bring the car in for inspection or can i send the photos first please let me know how to proceed thank you\n",
      "\n",
      "5.wav | Audio Duration: 24.10s | Transcription Time: 5.38s | WER: 0.045\n",
      "Prediction   : howt is the great way to practise pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "Ground Truth : loud is a great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n",
      "6.wav | Audio Duration: 55.82s | Transcription Time: 22.08s | WER: 0.076\n",
      "Prediction   : ing your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you don't need to speak fast in fact slower is easirly better just focus on making each word distinct and steady this full minute of speech is meant to help you taste transcription quality the more practice you get the better your clarity and fluency will be whether you're using vosq whisper or any other model the basic principle is the same speak clearly minimize background noise and be consistent you can read these passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you'll see great results in your voice recognizant experiments\n",
      "Ground Truth : your speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "7.wav | Audio Duration: 7.75s | Transcription Time: 1.40s | WER: 0.050\n",
      "Prediction   : te simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "Ground Truth : a simple sentence designed to be spoken clearly each word is easy to pronounce and understand when read aloud slowly\n",
      "\n",
      "8.wav | Audio Duration: 52.87s | Transcription Time: 31.62s | WER: 0.392\n",
      "Prediction   : is with tear sar sentences is important wen working with istis to tac models these models rely on gran adear and poper pronouncation to give etet results you dont latoispeak fast in fact slower is usually better just bocas on making is work distinct and stedy this full minute of i peece is made to help you tastran trancription quality the more pacticigat the better ocal clarity and fan eily whether you are using vorse whisper or any other model the basic principle is the same  speak clearly manimise better or nowis and be consistent you can red this pacis multipie thans ifnaret or even regard your own simplestories or experience the key is to isterelix and natural good or dealie to better transcription keep going and you will see great results in your voce regagins and recognison experiments\n",
      "Ground Truth : speech with clear short sentences is important when working with speech to text models these models rely on clean audio and proper pronunciation to give accurate results you dont need to speak fast in fact slower is usually better just focus on making each word distinct and steady this full minute of speech is meant to help you test transcription quality the more practice you get the better your cla clarity and fluency will be whether youre using vosk whisper or any other model the basic principle is the same speak clearly minimize background noise and  be consistent you can read this passage multiple times if needed or even record your own simple stories or experiences the key is to stay relaxed and natural good audio leads to better transcription keep going and you will see great results in your voice recognition experiments\n",
      "\n",
      "9.wav | Audio Duration: 23.95s | Transcription Time: 9.21s | WER: 0.175\n",
      "Prediction   : great way to practice pronunciation speaking clearly helps interpe understanding and communication this sort paragraph is filled with simple easy words you can repeat it multipied times to be to get better whisper and other spece modern models perform best when the adio is clean the the pace is steady and the pronunciation is consistent take your time while reading and try not to rust the word\n",
      "Ground Truth : great way to practice pronunciation speaking clearly helps improve understanding and communication this short paragraph is filled with simple easy words you can repeat it multiple times to get better whisper and other speech models perform best when the audio is clean the pace is steady and the pronunciation is consistent take your time while reading and try not to rush the words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_onnx_quantized = []\n",
    "\n",
    "for wav_file in sorted(os.listdir(test_data_dir)):\n",
    "    if not wav_file.endswith(\".wav\"):\n",
    "        continue\n",
    "\n",
    "    wav_path = os.path.join(test_data_dir, wav_file)\n",
    "    txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "    txt_path = os.path.join(transcribs_dir, txt_file)\n",
    "\n",
    "    if not os.path.isfile(txt_path):\n",
    "        print(f\"Ground truth missing for {wav_file}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Read ground truth transcription\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ground_truth = f.read().strip().lower()\n",
    "\n",
    "    # Load audio to get duration\n",
    "    speech_array, sr = sf.read(wav_path)\n",
    "    audio_duration = len(speech_array) / sr  # audio duration in seconds\n",
    "\n",
    "    # Transcribe with ONNX model and measure time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        prediction = transcribe_from_file(wav_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {wav_file}: {e}\")\n",
    "        continue\n",
    "    transcription_time = time.time() - start_time\n",
    "\n",
    "    # Compute WER\n",
    "    error_rate = wer(ground_truth, prediction)\n",
    "\n",
    "    # Print result\n",
    "    print(f\"{wav_file} | Audio Duration: {audio_duration:.2f}s | Transcription Time: {transcription_time:.2f}s | WER: {error_rate:.3f}\")\n",
    "    print(f\"Prediction   : {prediction}\")\n",
    "    print(f\"Ground Truth : {ground_truth}\\n\")\n",
    "\n",
    "    # Save result with updated keys\n",
    "    results_onnx_quantized.append({\n",
    "        \"model\": \"large_960h_lv60_quantized.onnx\",\n",
    "        \"audio_file_name\": wav_file,\n",
    "        \"audio_duration_sec\": round(audio_duration, 2),\n",
    "        \"transcription_time_sec\": round(transcription_time, 2),\n",
    "        \"wer\": round(error_rate, 4),\n",
    "        \"prediction\": prediction,\n",
    "        \"ground_truth\": ground_truth\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e52745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_onnx_wer_quantized(results_onnx_quantized):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    model_wer_stats = defaultdict(lambda: {\"total_wer\": 0.0, \"count\": 0, \"total_audio_duration\": 0.0, \"total_transcription_time\": 0.0})\n",
    "\n",
    "    for result in results_onnx_quantized:\n",
    "        model = result[\"model\"]\n",
    "        model_wer_stats[model][\"total_wer\"] += result[\"wer\"]\n",
    "        model_wer_stats[model][\"count\"] += 1\n",
    "        model_wer_stats[model][\"total_audio_duration\"] += result.get(\"audio_duration_sec\", 0)\n",
    "        model_wer_stats[model][\"total_transcription_time\"] += result.get(\"transcription_time_sec\", 0)\n",
    "\n",
    "    print(\"\\n=== Quantized ONNX Model Summary ===\")\n",
    "    for model, stats in model_wer_stats.items():\n",
    "        count = stats[\"count\"]\n",
    "        avg_wer = stats[\"total_wer\"] / count if count > 0 else 0\n",
    "        avg_audio_dur = stats[\"total_audio_duration\"] / count if count > 0 else 0\n",
    "        avg_trans_time = stats[\"total_transcription_time\"] / count if count > 0 else 0\n",
    "\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"  Number of files     : {count}\")\n",
    "        print(f\"  Average WER         : {avg_wer:.4f}\")\n",
    "        print(f\"  Average Audio Length: {avg_audio_dur:.2f} sec\")\n",
    "        print(f\"  Average Transcription Time: {avg_trans_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baefd4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Quantized ONNX Model Summary ===\n",
      "Model: large_960h_lv60_quantized.onnx\n",
      "  Number of files     : 10\n",
      "  Average WER         : 0.3421\n",
      "  Average Audio Length: 27.21 sec\n",
      "  Average Transcription Time: 9.85 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_quantized(results_onnx_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49f13947",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarize_onnx_wer_o' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msummarize_onnx_wer_o\u001b[49m(results_onnx)\n",
      "\u001b[31mNameError\u001b[39m: name 'summarize_onnx_wer_o' is not defined"
     ]
    }
   ],
   "source": [
    "summarize_onnx_wer_o(results_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2007c59",
   "metadata": {},
   "source": [
    "### Quantizing OpenVINO model using POT (Post-Training Optimization Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debede0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
